<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jong-min-moon.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jong-min-moon.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-11T23:50:05+00:00</updated><id>https://jong-min-moon.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Jongmin Mun. </subtitle><entry><title type="html">Dynamic Lot Sizing Problem</title><link href="https://jong-min-moon.github.io/blog/2026/dynamic-lot-sizing/" rel="alternate" type="text/html" title="Dynamic Lot Sizing Problem"/><published>2026-02-07T00:00:00+00:00</published><updated>2026-02-07T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2026/dynamic-lot-sizing</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2026/dynamic-lot-sizing/"><![CDATA[<p>The <strong>Dynamic Lot Sizing Problem</strong> is a fundamental model in inventory management where a decision-maker must determine the optimal order quantities for a single item over a finite planning horizon.</p> <h2 id="problem-definition">Problem Definition</h2> <p>A manager decides how much inventory to order in each period $t$ to satisfy demand while minimizing total costs, which typically include ordering costs and holding costs.</p> <h3 id="key-assumptions">Key Assumptions</h3> <ol> <li><strong>Single-Item, Single-Level</strong>: We consider only the end product, ignoring raw materials or multi-echelon interactions.</li> <li><strong>Finite Planning Horizon</strong>: The decision-making process spans a discrete, finite timeline $t = 1, 2, \dots, T$.</li> <li><strong>Known Dynamic Demand</strong>: The demand $d_t$ for each period varies over time (hence “dynamic”) but is known in advance for the entire sequence $d_1, \dots, d_T$ (deterministic).</li> <li><strong>Periodic Review</strong>: Inventory levels are reviewed, and ordering decisions are made at the beginning of each period.</li> <li><strong>Unconstrained Capacity</strong>: There are no limits on the order quantity or inventory storage (infinite warehouse assumption).</li> <li><strong>No Backorders</strong>: Demand must be fully met in the period it occurs; shortages are not permitted.</li> </ol> <p>Input variables:</p> <ul> <li>$d_t$: demand in period $t$</li> <li>$c_o$: ordering cost</li> <li>$c_h$: holding cost</li> </ul> <h2 id="calcium-imaging">Calcium Imaging</h2> <p>given y_1, …, y_T, want to decide s_1, …, s_T and c_1,.. c_T c_t: current inventory level. s_t: order quantity in period t. perishable good: c_t = s_t + \gamma c_(t-1) the holding cost is 1/2(y_t-c_t)^2.</p>]]></content><author><name></name></author><category term="operations-research"/><category term="optimization"/><summary type="html"><![CDATA[An introduction to the Dynamic Lot Sizing Problem and its key assumptions.]]></summary></entry><entry><title type="html">Fundamentals of Diffusion Map Embedding</title><link href="https://jong-min-moon.github.io/blog/2024/diffusion-map-embedding/" rel="alternate" type="text/html" title="Fundamentals of Diffusion Map Embedding"/><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2024/diffusion-map-embedding</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2024/diffusion-map-embedding/"><![CDATA[<h2 id="the-diffusion-map-workflow">The Diffusion Map Workflow<d-cite key="KrishnaswamyLab2022Diffusion"></d-cite></h2> <p><strong>Input:</strong> Data in ambient dimension (of any type that a kernel can be defined).</p> <ol> <li><strong>Pairwise Distance Matrix:</strong> an $n \times n$ matrix, $(i,j)$-th entry is the distance between $X_i$ and $X_j$.</li> <li><strong>Affinity Matrix:</strong> Perform an entry-wise evaluation using a kernel function (typically Gaussian) to convert distances into affinities. Affinity captures the <strong>local geometric structure</strong> of the data and ignores the global structure.</li> <li><strong>Transition Matrix ($P$):</strong> Row-normalize the affinity matrix to create the stochastic transition matrix $P$, often referred to as the <strong>diffusion operator</strong>. This matrix defines the probability of a “random walk” jumping from one data point to another in a single step.</li> <li><strong>Spectral Decomposition:</strong> Perform an eigendecomposition on $P$. We focus on the non-trivial eigenvectors ${\psi_1, \psi_2, \dots, \psi_k}$ (excluding the constant $\psi_0$). The $i$-th entry of each eigenvector, scaled by its corresponding eigenvalue $\lambda^t$, serves as the new coordinate for the $i$-th data point.</li> </ol> <p><strong>Output:</strong> $k$-dimensional Euclidean vectors representing the data on the underlying manifold.</p> <h2 id="distance-matrix">Distance Matrix</h2> <p>For Euclidean data, the entries of the distance matrix are defined as: \(D(x_i, x_j) = \sqrt{\|x_i - x_j\|^2}\) However, the data can be of any type, provided a suitable kernel can be defined to measure similarity. In the distance matrix below, we observe a banded pattern extending across the entire matrix. indicating a global structure.</p> <div class="fake-img l-body"> <img src="/assets/img/diffusion-map-embedding/dme_distance.png" alt="distance matrix" width="90%" style="display: block; margin: auto;"/> <div class="caption"> Distance matrix </div> </div> <h2 id="affinity-matrix">Affinity Matrix</h2> <p>The pairwise distances are then passed through a nonlinear kernel function to calculate affinities. The nonlinearity effectively preserves local geometric information by giving higher weight to nearby points. For example, using a Gaussian kernel: \(A(x_i, x_j) = \exp\left(-\frac{D(x_i, x_j)^2}{\sigma}\right)\) In this context, $\sigma$ (or $\epsilon$) acts as a scale parameter that determines the size of the local neighborhood. There are many other kernels other than Gaussian kernel.</p> <p>The Affinity matrix below show that global structure is partially removed (points are arranged according to the underlying swiss roll structure). The graph shows that there still exists unncessary edges.</p> <p>div class=”fake-img l-body”&gt; <img src="/assets/img/diffusion-map-embedding/dme_affinity.png" alt="affinity matrix" width="90%" style="display: block; margin: auto;"/></p> <div class="caption"> Affinity matrix </div> <p>&lt;/div&gt;</p> <h2 id="transition-matrix">Transition Matrix</h2> <p>The affinity matrix is then row-normalized to create the stochastic transition matrix $P = D^{-1} A$, the affinity matrix pre-multiplied by the degree inverse matrix. This matrix defines the probability of a “random walk” jumping from one data point to another in a single step.</p> <p>This transition matrix $P$ acts as a Diffusion Operator with several key properties:</p> <ol> <li>Markov Chains: Diffusion operators define Markov Chains.</li> <li>Assymetric: Because the operator is not necessarily symmetric, right eigenvectors ($Pu = \lambda u$) are generally unequal to left eigenvectors ($uP = \lambda u$).</li> <li>Steady State: The left eigenvector corresponding to $\lambda = 1$ is the steady state vector. In Markov chain all the eigenvalues are less than or equal to 1.</li> <li>Triviality: The right eigenvector corresponding to $\lambda = 1$ is trivial unless the graph is disconnected.</li> <li>Powering: Powering a diffusion operator is mathematically equivalent to powering only its eigenvalues: \(P^t = U \Lambda^t U^{-1}\)</li> </ol> <h3 id="heat-transfer">Heat Transfer</h3> <p>Why is $P$ called a diffusion operator? Because it diffuses heat! Since $P$ is a $n \times n$ matrix, it is a linear operator mapping a vector $f \in \mathbb{R}^n$ onto another vector in $\mathbb{R}^n$.</p> <ul> <li>Input: $f$ is a vector of length $n$, where each entry represents the “heat” at a specific data point. For example, if you put “1” at point $A$ and “0” everywhere else, you are starting with all the heat at point $A$.</li> <li>Operation: When you multiply the matrix $P$ by your vector ($P f$), the matrix acts as the diffusion operator. According to the transition probabilities in $P$, the heat at point $i$ is redistributed to its neighbors. Points that have high affinity (local similarity) receive more heat than points that are far away.</li> <li>Output: The output is a new $n$-vector, $f_{next} = Pf$. Each entry in this new vector contains the “updated” heat levels. If you repeat this $t$ times ($P^t f$), you are simulating the diffusion process over a specific time scale.</li> </ul> <h2 id="eigendecomposition-and-embedding">Eigendecomposition and Embedding</h2> <p>At this stage, diffusion map embedding also does eigendecomposition, but on a $n \times n$ matrix, not a $p \times p$ matrix. We compare the process with PCA.</p> <h3 id="1-principal-component-analysis-pca">1. Principal Component Analysis (PCA)</h3> <p>PCA operates on the <strong>feature space</strong>. We first compute the covariance matrix $\Sigma = \frac{1}{n-1}X^TX$, which is a $p \times p$ matrix. We then perform the eigendecomposition: \(\Sigma = U \Lambda U^T\) We retain the first $k$ components to form $U_k$. The low-dimensional embedding is then calculated via <strong>projection</strong> (inner product): \(X_{low} = XU_k\) By design, this imbedding preserves the covariance matrix.</p> <h3 id="2-diffusion-map-embedding">2. Diffusion Map Embedding</h3> <p>In contrast, Diffusion Maps operate on the <strong>sample space</strong>. Here, we use a stochastic transition matrix $M$, which is $n \times n$ (where $n$ is the number of observations). Now each observation $X_i$ need not be $p$-dimensional Euclidean. It can be of any data type for which a kernel can be defined.</p> <p>The eigenvector of $M$ is therefore an $n$-vector. For embedding, we do not project, but lookup and scale.</p> <ul> <li>For each observation $x_i$, we extract the $i$-th element from the first $k$ eigenvectors.</li> <li>After scaling by the eigenvalues $\lambda^t$ to account for the diffusion time, we arrive at the $k$-dimensional embedding: \(\Psi_t(x_i) = \left( \lambda_1^t \psi_1(i), \lambda_2^t \psi_2(i), \dots, \lambda_k^t \psi_k(i) \right)\)</li> </ul>]]></content><author><name>Jongmin Mun</name></author><category term="research"/><category term="computational-neuroscience"/><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Principles and practice of nonlinear dimensionality reduction popularly used in neuroscience.]]></summary></entry><entry><title type="html">Fundamentals of Matching and Weighting Estimator for Causal Inference</title><link href="https://jong-min-moon.github.io/blog/2024/matching/" rel="alternate" type="text/html" title="Fundamentals of Matching and Weighting Estimator for Causal Inference"/><published>2024-08-13T00:00:00+00:00</published><updated>2024-08-13T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2024/matching</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2024/matching/"><![CDATA[<h2 id="i-the-counterfactual-framework-rubin-causal-model">I. The Counterfactual Framework (Rubin Causal Model)</h2> <p>In the potential outcomes framework, for each unit $i$, we define:</p> <ul> <li><strong>Treatment ($D_i$):</strong> A binary indicator.</li> <li><strong>Covariates ($X_i$):</strong> A vector of pre-treatment characteristics.</li> <li><strong>Potential Outcomes:</strong> $Y_i(1), Y_i(0)$.</li> </ul> <p>The <strong>Individual Treatment Effect (ITE)</strong>: \(\tau_i = Y_i(1) - Y_i(0)\) is a random variable. It is not observed since we only observe one outcome for each unit: \(Y_i = D_i Y_i(1) + (1 - D_i) Y_i(0)\)</p> <p>Since we cannot calculate $\tau_i$ directly, we instead estimate <strong>Average Treatment Effect (ATE)</strong>: \(\tau_{ATE} = E[Y(1) - Y(0)]\)</p> <h2 id="ii-the-naive-estimator-and-selection-bias">II. The Naive Estimator and Selection Bias</h2> <h3 id="1-the-naive-comparison">1. The Naive Comparison</h3> <p>In observational studies where the treatment allocation probability $\pi$ is not equal to $0.5$, one might use the <strong>Naive Estimator</strong>: \(\hat{\delta}_{naive} = \hat{E}[Y | D=1] - \hat{E}[Y | D=0]\) which converges to \(E[Y(1) | D=1] - E[Y(0) | D=0]\) which consists of conditional expectation parameters. The counterfactual framework has four key conditional expectations (plus the treatment probability $\pi$):</p> <ol> <li> <table> <tbody> <tr> <td>$E[Y(1)</td> <td>D=1]$: Treated outcome for the treated.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$E[Y(0)</td> <td>D=1]$: Untreated outcome for the treated (<strong>Counterfactual</strong>).</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$E[Y(1)</td> <td>D=0]$: Treated outcome for the control (<strong>Counterfactual</strong>).</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$E[Y(0)</td> <td>D=0]$: Untreated outcome for the control.</td> </tr> </tbody> </table> </li> </ol> <p>The naive estimator consistently estimate 1 and 4. The treatment probability ($\hat{E}[D] \xrightarrow{p} \pi$) is also consistently estimated. So using a naive esitimator means using these three quantities. ATE is expressed by these five parameters as follows: \(\tau_{ATE} = \pi \underbrace{(E[Y(1)|D=1] - E[Y(0)|D=1])}_{\text{ATT}} + (1-\pi) \underbrace{(E[Y(1)|D=0] - E[Y(0)|D=0])}_{\text{ATU}}\) If we attempt to equate the Naive Estimator to the true ATE, the terms do not cancel out gracefully without some assumption because there are many possible values or 2 and 3 that the equation do not hold. However, we lack estimators for the two counterfactual terms (2 and 3).</p> <h3 id="assumptions">Assumptions</h3> <p>If we assume \(E[Y(1) | D=1] = E[Y(1) | D=0]\) \(E[Y(0) | D=1] = E[Y(0) | D=0]\) then the Naive Estimator is a valid estimator for ATE (plug those in to the expansion above to check this). This is a quite strong assumption. We can create many situations where $D$ and $Y(0), Y(1)$ are both functions of $X$, and (not thus) the assumption is violated.</p> <p>So we want to assume a milder assumption. For each value of $X$, there will be corresponding subpopulation. Let’s assume that $X$ is informative and granular enough that within each subpopulation, the treatment assignment is effectively random. This would be true if the treatment was assigned by Bernoulli trial with probability $\pi(X)$.</p> <p>Then, we only need to assume \(E[Y(1) | D=1, X] = E[Y(1) | D=0, X]\) \(E[Y(0) | D=1, X] = E[Y(0) | D=0, X].\)</p> <p>Under these assumptions, we can consistenly estiamte the CATE: \(\tau_{CATE}(X) = E[Y(1) | X] - E[Y(0) | X]\) \(= E[Y(1) | D=1, X] - E[Y(0) | D=0, X]\) \(= E[Y(1) | D=1, X] - E[Y(0) | D=1, X]\) \(= E[Y | D=1, X] - E[Y | D=0, X]\) by the conditional sample mean difference.</p> <p>By the law of iterated expecation, ATE is \(\tau_{ATE} = E[\tau_{CATE}(X)]\) Since CATE is a function of $X$, the expectation above is taken over $X$ with probability density function $f(x)$. Since we beleive that $X$ is generated from $f(X)$, empirical measure plug-in version is just sample mean of CATE values for each $X_i$ in the dataset. \(\hat{\tau}_{ATE} = \frac{1}{n} \sum_{i=1}^n \hat{\tau}_{CATE}(X_i)\)<br/> If we can compute $\hat{\tau}_{CATE}(X_i)$ for all $X_i$ in our dataset. So this is the basic idea for matching: compute for each covariate, and aggregate. Divide and conquer.</p>]]></content><author><name>Jongmin Mun</name></author><category term="research"/><category term="computational-neuroscience"/><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Principles and practice of matching and weighting estimator for causal inference.]]></summary></entry><entry><title type="html">Data Analysis Detail: Subsequent Analysis for Biological Interpretation</title><link href="https://jong-min-moon.github.io/blog/2023/fc-da/" rel="alternate" type="text/html" title="Data Analysis Detail: Subsequent Analysis for Biological Interpretation"/><published>2023-02-13T00:00:00+00:00</published><updated>2023-02-13T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2023/fc-da</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2023/fc-da/"><![CDATA[<p>We are studying how sex (male/female) and disease state (ASD vs. control) interact to affect brain function. We have successfully estimated a Sex x Disease interaction effect. In statistical terms, we have a vector $\mathbf{t}$ of length $P$ (where $P=7$, representing our 7 brain regions/ROIs). Each entry $t_i$ tells us the strength of the interaction in that specific region.</p> <p>But a vector of t-statistics is just a list of numbers. To make it biologically meaningful, we need to translate these numbers into biological and cognitive semantics. We asked: “If a brain region has a high interaction t-statistic, what cognitive functions or genes are associated with it?”</p> <div class="fake-img l-body"> <img src="/assets/img/publication_preview/neuroimage_t_statistic_map.png" alt="t-statistic map"/> <div class="caption"> t-statistic map of sex×disease interaction </div> </div> <p>Here is how we “decoded” the vector using external databases.</p> <ol> <li> <p><strong>Cognitive decoding</strong> → linking the statistical map to mental functions.</p> </li> <li> <p><strong>Gene enrichment analysis</strong> → linking it to patterns of gene expression in the brain.</p> </li> </ol> <h2 id="cognitive-decoding">Cognitive Decoding</h2> <p>To interpret our vector $\mathbf{t}$, we used NeuroSynth, which is essentially a massive, NLP-mined contingency table of brain activity.</p> <ul> <li>Data: NeuroSynth database is a giant matrix $\mathbf{C}$ of size $P \times K$. <ul> <li>Rows (P): Brain regions (our 7 ROIs).</li> <li>Columns (K): Cognitive terms (e.g., “Memory”, “Pain”, “Social”, “Motor”).</li> <li>Values: The probability that region i is active given the term k appears in a study \(C_{i,k} = P(\text{region } i \text{ is active } | \text{ term } k \text{ appears in a study})\)</li> </ul> </li> <li> <p>Decoding (Correlation): We simply calculated the correlation between our interaction vector $\mathbf{t}$ and every column in $\mathbf{C}$. \(r_k = \text{Corr}(\mathbf{t}, \mathbf{C}_{., k})\)</p> </li> <li><strong>Result:</strong> <ol> <li><strong>Global Alignment (Concept-independent):</strong> Our interaction map showed a strong correlation with the <strong>First Principal Component (PC1) of NeuroSynth</strong>. This PC represents the “gist” of the functional hierarchy—the primary axis of spatial variation in brain function. A high correlation confirms that our interaction effect is strongest in regions with high PC scores (meaningful functional networks), rather than random noise.</li> <li><strong>Specific Cognitive Functions:</strong> We subsequently found the highest correlations ($\approx 0.77$) with specific cognitive terms: <strong>“Social”</strong>, <strong>“Moral”</strong>, <strong>“Theory of Mind”</strong>, <strong>“Autobiographical”</strong>, and <strong>“Self-referential”</strong>.</li> </ol> </li> <li><strong>Interpretation:</strong> The spatial distribution of the Sex x ASD interaction is nearly identical to the spatial distribution of the brain’s “social network”. The interaction effect isn’t random noise; it’s structurally aligned with high-level cognition.</li> </ul> <div class="fake-img l-body"> <img src="/assets/img/neuroimage_principal_score_neurosynth.png" alt="neuroimage principal score neurosynth"/> <div class="caption"> Correlation between t-statistic map and principal score of neurosynth </div> </div> <div class="fake-img l-body"> <img src="/assets/img/neuroimage_NLP.png" alt="neuroimage NLP"/> <div class="caption"> Correlation between t-statistic map and cognitive terms of neurosynth </div> </div> <h2 id="gene-enrichment-transcriptomic-analysis">Gene Enrichment (transcriptomic) Analysis</h2> <h3 id="goal">Goal</h3> <p>Determine if the spatial pattern of Sex × ASD interaction ($\mathbf{t}$) corresponds to specific gene expression profiles.</p> <h3 id="method">Method</h3> <ol> <li><strong>Data Source:</strong> We used <strong>abagen</strong> to process microarray gene-expression data from the <strong>Allen Human Brain Atlas</strong>, generating a region-by-gene matrix ($\mathbf{G}$ of size $P \times M$).</li> <li><strong>Correlation:</strong> We correlated our interaction vector $\mathbf{t}$ with the spatial expression map of each gene. Genes with strong spatial similarity were identified as “top hits.”</li> <li><strong>Enrichment:</strong> We applied a <strong>developmental enrichment tool (CSEAtool)</strong> to determine if these top genes are over-represented in specific brain structures or developmental stages.</li> </ol> <h3 id="findings">Findings</h3> <p>The genes associated with the interaction effect were significantly enriched in the cortex, striatum, and thalamus during development. Previous studies showed an increase in the growth rate of striatal structures in ASD, a reduction in thalamic volume, and dysregulation of thalamocortical networks. Therefore, this confirms again our t-statistic is high in ASD-related regions.</p> <h3 id="summary-for-statisticians">Summary for Statisticians</h3> <p>We treated the biological interpretation as a pattern matching problem.</p> <ol> <li>Input: A target vector $\mathbf{t}$ (the interaction t-statistics).</li> <li>Reference: External high-dimensional matrices (Cognitive definition s $\mathbf{C}$ and Gene expression $\mathbf{G}$).</li> <li>Method: Spatial correlation ($\rho$) to identify the feature columns in $\mathbf{C}$ and $\mathbf{G}$ that share the same topology as $\mathbf{t}$.</li> </ol> <p>This allowed us to move from “We found a significant t-test” to “The difference is driven by social processing circuits formed during adolescence”.</p>]]></content><author><name>Jongmin Mun</name></author><category term="research"/><category term="computational-neuroscience"/><category term="paper-details"/><category term="real-data-analysis"/><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Principle and practice of mapping the brain's functional architecture into a low-dimensional space.]]></summary></entry><entry><title type="html">Fundamentals of Functional Connectivity and Gradients</title><link href="https://jong-min-moon.github.io/blog/2023/fc/" rel="alternate" type="text/html" title="Fundamentals of Functional Connectivity and Gradients"/><published>2023-02-09T00:00:00+00:00</published><updated>2023-02-09T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2023/fc</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2023/fc/"><![CDATA[<h2 id="i-functional-connectivity">I. Functional Connectivity</h2> <p>At the level of basic measurements, neuroimaging data can be said to consist typically of a set of signals (usually time series) at each of a collection of pixels (in two dimensions) or voxels (in three dimensions). Building from such data, various forms of higher-level data representations are employed in neuroimaging<d-cite key="ginestetHypothesisTestingNetwork2017"></d-cite>. Since brain regions cooperate within large-scale functional networks to support specific cognitive processes, increasingly in recent years, there has emerged a substantial interest in network-based representations<d-cite key="warnickBayesianApproachEstimating2018"></d-cite>.</p> <p>There are three main types of brain connectivity:</p> <table> <thead> <tr> <th style="text-align: left">Connectivity Type</th> <th style="text-align: left">Description</th> <th style="text-align: left">Modality</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Functional Connectivity</strong></td> <td style="text-align: left">Investigates the <strong>undirected</strong> statistical dependence between separate brain regions characterized by similar temporal dynamics. It does not imply direct structural connection.</td> <td style="text-align: left">fMRI, EEG, MEG</td> </tr> <tr> <td style="text-align: left"><strong>Effective Connectivity</strong></td> <td style="text-align: left">Refers to the <strong>directed, or causal</strong> influence of one region over another.</td> <td style="text-align: left">fMRI, EEG, MEG, TMS</td> </tr> <tr> <td style="text-align: left"><strong>Structural Connectivity</strong></td> <td style="text-align: left">Investigates the <strong>physical connections</strong> (e.g., axons, white matter) between brain regions.</td> <td style="text-align: left">DTI, sMRI</td> </tr> </tbody> </table> <p>Functional connectivities are inferred when signals from two separate regions fluctuate in synchrony. Basically, computing correlation. Signals can be from any modalities. For example:</p> <ul> <li><strong>fMRI:</strong> Functional magnetic resonance imaging (fMRI) provides an indirect measure of neuronal activity by evaluating changes in blood oxygenation over different areas of the brain. Good spatial resolution, low temporal resolution.</li> <li><strong>EEG / MEG:</strong> Measures <strong>electrical/magnetic activity</strong> directly. Excellent temporal resolution, lower spatial resolution.</li> </ul> <h2 id="ii-functional-gradients-low-dimensional-representation">II. Functional Gradients: Low-Dimensional Representation</h2> <h3 id="1-the-curse-of-dimensionality">1. The Curse of Dimensionality</h3> <ul> <li><strong>The Problem:</strong> Functional connectivity analysis generates a $p \times p$ matrix, where $p$ is the number of brain regions. If $p=400$, each region is defined by a complex 400-dimensional connectivity vector. This high-dimensional space is noisy, difficult to visualize, and computationally expensive to model.</li> <li><strong>The Solution:</strong> Dimensionality reduction techniques (e.g., PCA, t-SNE, Diffusion Mapping) are applied to extract the “dominant axes” of variance. These axes represent the underlying organizational principles of the brain.</li> </ul> <h3 id="2-why-called-gradient">2. Why called “Gradient”?</h3> <p>A <strong>Functional Gradient</strong> is analogous to a Principal Component (PC) score, serving as a continuous coordinate system for the brain. Rather than categorizing regions into discrete clusters, a gradient positions each region along a continuous spectrum determined by the similarity of its connectivity profile.</p> <h3 id="3-why-use-functional-gradients">3. Why use Functional Gradients?</h3> <p>It is low-dimensional but encodes the brain activity sufficiently well. For example, gradients is shown to distinguish between primary sensory/motor regions (e.g., Visual Cortex) that process immediate external stimuli and association regions (e.g., Default Mode Network) involved in abstract, internal cognition like memory and planning.</p> <h2 id="iii-data-acquisition-workflow">III. Data Acquisition Workflow</h2> <p>This workflow moves beyond simple “Region A connects to Region B” analysis to map the macro-scale hierarchy of the brain.</p> <ol> <li><strong>Construct Connectivity Matrix:</strong> <ul> <li><em>Input:</em> Time-series data for $p$ regions.</li> <li><em>Operation:</em> Pearson correlation between all pairs.</li> <li><em>Result:</em> <strong>Symmetric, Dense Matrix</strong>.</li> </ul> </li> <li><strong>Row-wise Thresholding:</strong> <ul> <li><em>Operation:</em> For each row, keep only the top 10% strongest connections.</li> <li><em>Result:</em> <strong>Sparse, Asymmetric Matrix</strong>. (Asymmetry arises because being in A’s top 10% doesn’t guarantee A is in B’s top 10%).</li> <li><em>Purpose:</em> Filter out noise and weak correlations.</li> </ul> </li> <li><strong>Affinity Matrix Calculation:</strong> <ul> <li><em>Operation:</em> Compute <strong>Cosine Similarity</strong> between the sparse rows (connectivity profiles).</li> <li><em>Question:</em> “Do Region A and Region B have similar ‘friends’ (connectivity patterns)?”</li> <li><em>Result:</em> <strong>Symmetric Affinity Matrix</strong>. represents <em>contextual similarity</em>, not direct correlation.</li> </ul> </li> <li><strong>Dimensionality Reduction:</strong> <ul> <li><em>Operation:</em> Apply <strong>Diffusion Map Embedding</strong>.</li> <li><em>Result:</em> <strong>Principal Gradients</strong>. Each region gets a coordinate/score along these axes.</li> </ul> </li> </ol>]]></content><author><name>Jongmin Mun</name></author><category term="research"/><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Principle and practice of mapping the brain's functional architecture into a low-dimensional space.]]></summary></entry><entry><title type="html">Wavelet Basis &amp;amp; Function Spaces: Besov vs. Hölder</title><link href="https://jong-min-moon.github.io/blog/2022/DWT/" rel="alternate" type="text/html" title="Wavelet Basis &amp;amp; Function Spaces: Besov vs. Hölder"/><published>2022-10-31T00:00:00+00:00</published><updated>2022-10-31T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2022/DWT</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2022/DWT/"><![CDATA[<h3 id="i-context-signal-processing-vs-statistical-estimation">I. Context: Signal Processing vs. Statistical Estimation</h3> <p>Before diving into Besov spaces, it is crucial to understand the fundamental shift from <strong>Convolution</strong> (used in neuroscience/signal processing) to <strong>Basis Expansion</strong> (used in statistics).</p> <table> <thead> <tr> <th style="text-align: left">Feature</th> <th style="text-align: left"><strong>Wavelet Convolution (CWT)</strong></th> <th style="text-align: left"><strong>Wavelet Basis (DWT)</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Goal</strong></td> <td style="text-align: left">Phase, Power Extraction &amp; Visualization</td> <td style="text-align: left">Estimation, Compression, Denoising</td> </tr> <tr> <td style="text-align: left"><strong>Method</strong></td> <td style="text-align: left">Sliding window (Redundant)</td> <td style="text-align: left">Tiling / Grid (Orthogonal)</td> </tr> <tr> <td style="text-align: left"><strong>Structure</strong></td> <td style="text-align: left">Smooth, overlapping coefficients</td> <td style="text-align: left">Sparse, independent coefficients</td> </tr> <tr> <td style="text-align: left"><strong>Wavelet</strong></td> <td style="text-align: left">Complex Morlet (Smooth)</td> <td style="text-align: left">Haar, Daubechies (Compact/Step)</td> </tr> </tbody> </table> <p>In the statistical context (e.g., nonparametric regression), we prioritize <strong>efficiency</strong> and <strong>sparsity</strong>. We want to reconstruct a function $f(t)$ using the fewest number of coefficients possible.</p> <hr/> <h3 id="ii-the-haar-wavelet-basis">II. The Haar Wavelet Basis</h3> <p>The text provided focuses on the <strong>Haar Multivariate Wavelet Basis</strong>. This system creates a hierarchical representation of a function using step-like “building blocks.”</p> <h4 id="1-the-structure-setting-j0">1. The Structure (Setting $J=0$)</h4> <p>The analysis starts at the coarsest level ($J=0$), dividing the function into two categories:</p> <ul> <li><strong>Scaling Coefficients ($\theta_\phi$):</strong> <ul> <li><strong>Symbol:</strong> $\Phi_0$</li> <li><strong>Role:</strong> Represents the <strong>Global Trend</strong> or average of the function over the domain $[0,1]^d$. The coefficient is simply just the mean of the function.</li> </ul> </li> <li><strong>Wavelet Coefficients ($\theta_\psi$):</strong> <ul> <li><strong>Symbol:</strong> $\Psi_j$ (for levels $j \ge 0$)</li> <li><strong>Role:</strong> Captures <strong>Abrupt Oscillations</strong> and details that deviate from the global trend.</li> <li><strong>Resolution:</strong> As level $j$ increases, the wavelets become narrower and taller, capturing higher-frequency details.</li> </ul> </li> </ul> <h4 id="2-why-haar">2. Why Haar?</h4> <p>The Haar basis is chosen because projecting a density onto this basis is mathematically equivalent to <strong>Equal-Sized Binning</strong> or histogramization. The resolution level decides the bin width and the coefficients are the bin heights. This allows researchers to link abstract function space theory directly to the discretization error inherent in statistical testing.</p> <hr/> <h3 id="iii-the-besov-ball-a-budgeting-game">III. The Besov Ball: A “Budgeting Game”</h3> <p>How do we define if a function is “smooth”? The <strong>Besov Norm</strong> ($|||f|||_{s,2,q}$) measures smoothness by calculating the “cost” of building the function using these wavelet blocks.</p> \[|||f|||_{s,2,q} := \left[ \sum_{j=0}^{\infty} \underbrace{2^{jsq}}_{\text{Price Tag}} \underbrace{\left( \sum_{\psi \in \Psi_j} |\theta_{\psi}(f)|^2 \right)^{q/2}}_{\text{Energy at Level } j} \right]^{1/q}\] <h4 id="1-the-price-tag-2js">1. The “Price Tag” ($2^{js}$)</h4> <p>This term acts as a weighted penalty.</p> <ul> <li><strong>Low $j$ (Coarse levels):</strong> Cheap. You can use these blocks freely.</li> <li><strong>High $j$ (Fine details):</strong> Expensive. The cost grows exponentially ($2^{js}$).</li> </ul> <h4 id="2-the-rule-of-the-besov-ball">2. The Rule of the Besov Ball</h4> <p>To stay inside the Besov Ball (i.e., to have a finite norm), you must be “thrifty.” You are allowed to use high-frequency wavelets (high $j$), but you must use them <strong>sparingly</strong> (Sparsity).</p> <p>This definition allows the Besov space to accommodate <strong>Spatially Inhomogeneous</strong> functions—functions that are smooth in most places but have occasional sharp spikes or jumps.</p> <hr/> <h3 id="iv-comparison-besov-bs_pq-vs-hölder-cs">IV. Comparison: Besov ($B^s_{p,q}$) vs. Hölder ($C^s$)</h3> <p>This is the critical distinction for statistical modeling.</p> <h4 id="1-the-hölder-class-cs">1. The Hölder Class ($C^s$)</h4> <ul> <li><strong>Philosophy:</strong> <strong>Uniform Regularity</strong> (Worst-Case).</li> <li><strong>The Rule:</strong> The function must be smooth <strong>everywhere</strong>.</li> <li><strong>Sensitivity:</strong> If the function has a single “bad” point (a sharp corner, a jump, a spike) <em>anywhere</em>, the entire function is rejected.</li> <li><strong>Norm Analogy:</strong> Based on $L^\infty$ (Maximum error).</li> </ul> <h4 id="2-the-besov-space-bs_pq">2. The Besov Space ($B^s_{p,q}$)</h4> <ul> <li><strong>Philosophy:</strong> <strong>Average Regularity</strong>.</li> <li><strong>The Rule:</strong> The function must be smooth <strong>on average</strong>.</li> <li><strong>Flexibility:</strong> It tolerates local irregularities (like jumps or spikes) as long as they are spatially sparse. The “cost” of one bad point is averaged out over the smooth regions.</li> <li><strong>Norm Analogy:</strong> Based on $L^p$ (Integrated error).</li> </ul> <h4 id="3-specific-different-functions">3. Specific “Different Functions”</h4> <p>Because of this flexibility, Besov spaces contain functions that are banned from Hölder classes:</p> <table> <thead> <tr> <th style="text-align: left">Function Type</th> <th style="text-align: left">Description</th> <th style="text-align: left">Hölder Class ($C^s$)</th> <th style="text-align: left">Besov Space ($B^s_{p,q}$)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Step Function</strong></td> <td style="text-align: left">Flat $\rightarrow$ Jump $\rightarrow$ Flat</td> <td style="text-align: left"><strong>REJECTED</strong> (Infinite derivative at jump)</td> <td style="text-align: left"><strong>ACCEPTED</strong> (Jump requires only sparse coefficients)</td> </tr> <tr> <td style="text-align: left"><strong>Local Spike</strong></td> <td style="text-align: left">Smooth line with one sharp burst</td> <td style="text-align: left"><strong>REJECTED</strong> (Fails at the burst location)</td> <td style="text-align: left"><strong>ACCEPTED</strong> (Averaged out by smooth regions)</td> </tr> </tbody> </table> <h3 id="v-summary">V. Summary</h3> <p>By selecting the <strong>Haar Basis</strong> and <strong>Besov Spaces</strong>, one explicitly chooses a framework that can model <strong>discontinuities</strong> and <strong>abrupt changes</strong>. Unlike Fourier/Hölder methods, which assume data is uniformly smooth (like a sine wave), Besov/Haar methods assume data may have sharp edges and handle them robustly without “blowing up” the error metric.</p>]]></content><author><name></name></author><category term="research"/><category term="statistics"/><summary type="html"><![CDATA[Understanding Nonparametric Regression, Haar Wavelets, and Smoothness Norms]]></summary></entry><entry><title type="html">Data Analysis Detail: Mouse Behavioral Study</title><link href="https://jong-min-moon.github.io/blog/2021/maze/" rel="alternate" type="text/html" title="Data Analysis Detail: Mouse Behavioral Study"/><published>2021-11-29T00:00:00+00:00</published><updated>2021-11-29T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2021/maze</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2021/maze/"><![CDATA[<h2 id="study-objective">Study Objective</h2> <p>We aim to demonstrate the recording efficacy of the neural interface system developed by Park et al. (2024) in detecting individual and population level neural activities in a freely behaving mouse model.</p> <h2 id="target-brain-regions">Target Brain Regions</h2> <p>We implant neural brob in two distnct, functionally specialized brain regions to correlate neural activity with specific behavioral states.</p> <table> <thead> <tr> <th style="text-align: left">Target Region</th> <th style="text-align: left">Anatomical Location</th> <th style="text-align: left">Functional Association</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>CA1</strong></td> <td style="text-align: left">Hippocampus (Cornu Ammonis 1)</td> <td style="text-align: left">Navigation, spatial memory, and active locomotion.</td> </tr> <tr> <td style="text-align: left"><strong>L6</strong></td> <td style="text-align: left">Primary Visual Cortex (Layer 6)</td> <td style="text-align: left">Visual processing and head-rotation motion.</td> </tr> </tbody> </table> <div class="fake-img l-body"> <img src="/assets/img/mouse_brain_region_LA6_C1.png" alt="Mouse Brain Regions"/> <div class="caption"> Target brain regions: CA1 (green) and L6 (blue) </div> </div> <h2 id="mouse-task">Mouse task</h2> <p>Freely moving mice were placed at the start of the long arm of a T-shaped maze. As they traversed the track during the <strong>active locomotion</strong> phase, we monitored neural activity, specifically anticipating the activation of place units in the <strong>hippocampus (CA1)</strong>. When the mice reached the intersection, they transitioned to a <strong>stopping and orienting</strong> phase, rotating their heads to scan for visual patterns marking the exit path. This visual discrimination task was expected to drive neuronal activity in the <strong>primary visual cortex (L6)</strong>. Simultaneously, one neural probe recorded from CA1 and another from L6 to capture these region-specific dynamics.</p> <div class="fake-img l-body"> <img src="/assets/img/mouse_task.png" alt="Mouse Task"/> <div class="caption"> Mouse task: T-maze </div> </div> <table> <thead> <tr> <th style="text-align: left">Task Phase</th> <th style="text-align: left">Behavior</th> <th style="text-align: left">Relevant Brain Region</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Task 1: Locomotion</strong></td> <td style="text-align: left">Running along the long arm</td> <td style="text-align: left">Hippocampus (CA1)</td> </tr> <tr> <td style="text-align: left"><strong>Task 2: Orientation</strong></td> <td style="text-align: left">Stopping and head rotation at intersection</td> <td style="text-align: left">Visual Cortex (L6)</td> </tr> </tbody> </table> <hr/> <h3 id="iii-task-1-active-locomotion-analysis">III. Task 1: Active Locomotion Analysis</h3> <h4 id="1-correlation-between-neural-activity-and-behavior-visual-inspection">1. Correlation between Neural Activity and Behavior: visual inspection</h4> <p>To validate the signal fidelity of the probe, we examined how neural activity modulates during locomotion. Reliable recordings should exhibit distinct physiological changes at both the population and individual neuron levels when the mouse transitions from a stopped state to active running. Our data analysis confirms these expected patterns.</p> <table> <thead> <tr> <th style="text-align: left">Signal Level</th> <th style="text-align: left">Measurement Method</th> <th style="text-align: left">Behavioral Modulation (Running vs. Stopped)</th> <th style="text-align: left">Evidence</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Population (LFP)</strong></td> <td style="text-align: left"><strong>Theta Band (4-8 Hz):</strong> Extracted via low-pass filtering.</td> <td style="text-align: left"><strong>Amplitude:</strong> Increases significantly during running.<br/><strong>Phase:</strong> Trace becomes “denser” (indicating higher frequency and regularity).</td> <td style="text-align: left">Figure 4e,f vs 4h,j</td> </tr> <tr> <td style="text-align: left"><strong>Individual (Spikes)</strong></td> <td style="text-align: left"><strong>Firing Rate:</strong> Extracted via high-pass filtering (&gt;300 Hz) and spike sorting.</td> <td style="text-align: left"><strong>Rate:</strong> Increases during running, confirming the probe captures behaviorally modulated units.</td> <td style="text-align: left">Figure 4g vs Figure 4d</td> </tr> </tbody> </table> <hr/> <h4 id="2-statistical-correlation-spike-field-phase-locking">2. Statistical Correlation: Spike-Field Phase Locking</h4> <p>In contrast to the visual inspection of behavioral modulation, this analysis employed formal statistical tools—Spectral Clustering and Goodness-of-Fit tests—to strictly quantify the relationship between individual spikes and the population LFP. This validates that the recorded signals are not random noise.</p> <table> <thead> <tr> <th style="text-align: left">Analysis Step</th> <th style="text-align: left">Methodology</th> <th style="text-align: left">Results</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>1. Clustering</strong></td> <td style="text-align: left">Applied <strong>Spectral Clustering</strong> (PCA + k-means) to sorted spikes.</td> <td style="text-align: left">Identified <strong>two distinct clusters</strong>, suggesting two separate neuronal units with unique waveform patterns.</td> </tr> <tr> <td style="text-align: left"><strong>2. Phase Extraction</strong></td> <td style="text-align: left">Extracted the phase of the LFP (Theta banad) at the exact timestamp of each spike.</td> <td style="text-align: left">Generated a sequence of phase angles for each cluster.</td> </tr> <tr> <td style="text-align: left"><strong>3. Statistical Test</strong></td> <td style="text-align: left">Performed a <strong>Goodness-of-Fit test</strong> (vs. Uniform Distribution).</td> <td style="text-align: left"><strong>Significant p-values:</strong> $p=0.001$ (Cluster 1) and $p=0.002$ (Cluster 2).</td> </tr> <tr> <td style="text-align: left"><strong>4. Distribution</strong></td> <td style="text-align: left">Analyzed the distribution of phase angles.</td> <td style="text-align: left"><strong>Unimodal:</strong> Centered at ~330° (cluster 1) and ~45° (cluster 2) (figures 4-l,m).</td> </tr> </tbody> </table> <p><strong>Conclusion:</strong> The statistically significant phase locking and different distribution form across clusters confirms that the isolated units are not random noise but are fundamentally coupled to the local neural network. This provides strong evidence that the spike sorting process successfully extracted real, physiologically relevant neuronal signals.</p> <h3 id="task-2-directional-decoding-in-visual-cortex-v1-l6">Task 2: Directional Decoding in Visual Cortex (V1 L6)</h3> <p>While Task 1 focused on locomotion in CA1, Task 2 investigated how the <strong>Primary Visual Cortex (Layer 6)</strong> encodes visual information relative to the observer’s own movement—specifically, <strong>head rotation</strong>.</p> <p><strong>The Biological Context:</strong> To correctly identify a moving object, the brain must distinguish between “the object moving” and “the eyes moving.” The L6 region integrates visual stimuli with the observer’s motion status (head rotation) to create a stable perception of the world.</p> <h4 id="1-signal-quality-verification-spike-sorting">1. Signal Quality Verification (Spike Sorting)</h4> <p>The researchers implanted two neural probes in adjacent sites within the L6 region. Using PCA clustering, they successfully isolated four distinct single-unit waveforms. The separation metrics indicated extremely high signal fidelity:</p> <ul> <li><strong>Isolation Distance:</strong> 268.6 and 102.8 (Values &gt;20 are generally considered good).</li> <li><strong>L-ratio:</strong> 0.0046 and 0.006 (Extremely low values indicate minimal noise contamination).</li> </ul> <h4 id="2-lfp-dynamics-the-slope-of-the-signal">2. LFP Dynamics: The Slope of the Signal</h4> <p>The researchers monitored the <strong>Local Field Potential (LFP)</strong> traces during head-rotating behaviors. They observed a distinct linear relationship between the signal’s slope and the direction of the turn:</p> <table> <thead> <tr> <th style="text-align: left">Movement Direction</th> <th style="text-align: left">Head Rotation</th> <th style="text-align: left">Neural State</th> <th style="text-align: left">LFP Characteristic (Linear Fit)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Left Turn</strong></td> <td style="text-align: left">Counter-Clockwise</td> <td style="text-align: left"><strong>Depolarization</strong></td> <td style="text-align: left"><strong>Negative Slope</strong> (Decreasing voltage)</td> </tr> <tr> <td style="text-align: left"><strong>Right Turn</strong></td> <td style="text-align: left">Clockwise</td> <td style="text-align: left"><strong>Hyperpolarization</strong></td> <td style="text-align: left"><strong>Positive Slope</strong> (Signal increment)</td> </tr> </tbody> </table> <h4 id="3-single-unit-specificity-directional-tuning">3. Single-Unit Specificity (Directional Tuning)</h4> <p>Beyond the population-level LFP, individual neurons showed distinct directional biases. When the spike ratios of the PCA-clustered neurons were compared between left-turn and right-turn behaviors:</p> <ul> <li><strong>Cluster 1 &amp; 2 Neurons:</strong> Showed a strong spiking bias toward <strong>Left-Turn</strong> events (consistent with the depolarization observed in the LFP).</li> <li><strong>Cluster 4 Neurons:</strong> Showed the opposite bias, firing preferentially during <strong>Right-Turn</strong> events.</li> </ul> <h4 id="4-conclusion">4. Conclusion</h4> <p>These results demonstrate that the neural interface can detect the <strong>contextual dependency</strong> of visual processing. The neurons in L6 do not merely respond to visual input; they modulate their firing based on the subject’s motion status (head rotation). This aligns with established neurophysiological theories that the visual cortex processes “visual motion” by integrating it with the observer’s own movement trajectory.</p>]]></content><author><name>Jongmin Mun</name></author><category term="research"/><category term="computational-neuroscience"/><category term="paper-details"/><category term="real-data-analysis"/><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Details for the data analysis section of the Nature Communications paper 'In-vivo integration of soft neural probes through high-resolution printing of liquid electronics on the cranium']]></summary></entry><entry><title type="html">Fundamentals of Neural Signal Decomposition</title><link href="https://jong-min-moon.github.io/blog/2021/bands/" rel="alternate" type="text/html" title="Fundamentals of Neural Signal Decomposition"/><published>2021-11-22T00:00:00+00:00</published><updated>2021-11-22T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2021/bands</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2021/bands/"><![CDATA[<h3 id="i-neural-recording-is-a-mixture">I. Neural recording is a mixture</h3> <p>When a microelectrode is inserted into brain tissue, the voltage trace it records contains multiple frequencies simultaneously. These components can be separated through signal-processing techniques. This concept is analogous to a radio: while many stations broadcast simultaneously, they can be isolated by tuning into the specific frequency bands in which they transmit information.</p> <h3 id="ii-signal-separation-individual-vs-population-activity">II. Signal Separation: Individual vs. Population Activity</h3> <p>Unlike EEG, which captures distant signals from the scalp, a microelectrode inserted directly into brain tissue sits in close proximity to neurons. As a result, the recorded signal represents a mixture of activity at two distinct scales: single-unit spikes from individual neurons and aggregate population activity.</p> <p>To make sense of this, consider the electrode as a radio receiver. Just as a radio picks up a chaotic mix of signals where each station occupies a distinct frequency band, the raw neural recording contains multiple physiological phenomena operating at distinct frequencies. By applying digital filters, we first separate this raw trace into two fundamental components:</p> <ol> <li><strong>Low-Frequency (&lt;300 Hz):</strong> Represents <strong>LFP</strong> (Local Field Potential).</li> <li><strong>High-Frequency (&gt;300 Hz):</strong> Represents <strong>Spikes</strong> (Single-Neuron Action Potentials).</li> </ol> <p>The following table highlights the fundamental differences between these two signal types.</p> <table> <thead> <tr> <th style="text-align: left">Feature</th> <th style="text-align: left"><strong>Local Field Potential (LFP)</strong></th> <th style="text-align: left"><strong>Single-Neuron Spikes</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>What it Measures</strong></td> <td style="text-align: left">Summed electrical activity from <em>many</em> nearby neurons.</td> <td style="text-align: left">Rapid action potentials from <em>individual</em> neurons.</td> </tr> <tr> <td style="text-align: left"><strong>Physiological Origin</strong></td> <td style="text-align: left"><strong>Synaptic Inputs:</strong> Current flowing into/out of dendrites (subthreshold activity).</td> <td style="text-align: left"><strong>Neuronal Outputs:</strong> Voltage-gated sodium channel firing (action potentials).</td> </tr> <tr> <td style="text-align: left"><strong>Frequency Range</strong></td> <td style="text-align: left"><strong>Slow:</strong> 1 – 300 Hz</td> <td style="text-align: left"><strong>Fast:</strong> 300 – 5000 Hz (Events ≈ 1 ms)</td> </tr> <tr> <td style="text-align: left"><strong>Spatial Range</strong></td> <td style="text-align: left"><strong>Wide:</strong> ~0.5 – 1 mm radius (hundreds of neurons).</td> <td style="text-align: left"><strong>Narrow:</strong> ~50 – 100 $\mu$m radius (very close to tip).</td> </tr> <tr> <td style="text-align: left"><strong>Information Content</strong></td> <td style="text-align: left">Network-level dynamics, oscillations (Theta, Gamma), population coordination.</td> <td style="text-align: left">Precise firing rates, spike timing, stimulus selectivity, neural coding.</td> </tr> <tr> <td style="text-align: left"><strong>Analysis Methods</strong></td> <td style="text-align: left">Power spectra, Coherence, Phase synchronization.</td> <td style="text-align: left">Spike sorting, Tuning curves, Temporal coding.</td> </tr> </tbody> </table> <hr/> <p><strong>A Note on Frequency Scales:</strong> To intuitively grasp these timescales, consider that even 50 Hz is perceived as continuous. A four-cylinder car engine running at 3,000 RPM cycles at exactly 50 Hz (Gamma band). At this speed, you hear a continuous “hum” rather than discrete engine strokes. The 300 Hz cutoff is six times faster than this engine hum, effectively separating the slow, rhythmic background (LFP) from the extremely rapid, discrete events of neuronal firing (Spikes).</p> <h3 id="iii-high-frequency-component-spikes">III. High-Frequency Component (Spikes)</h3> <ul> <li><strong>Source:</strong> When a specific neuron close to the electrode fires, it creates a brief, high-amplitude voltage deflection.</li> <li><strong>Processing:</strong> Extracted using a <strong>High-Pass Filter</strong> (e.g., cut-off at 300 Hz).</li> <li><strong>Analogy:</strong> The voice of the person sitting right next to you. You hear their specific words and timing distinct from the background roar.</li> </ul> <h3 id="iv-low-frequency-component-lfp">IV. Low-Frequency Component (LFP)</h3> <ul> <li><strong>Source:</strong> As neurons receive input, currents flow into dendrites. These currents overlap and add up spatially, producing a smooth, slow-changing signal.</li> <li><strong>Processing:</strong> Extracted using a <strong>Low-Pass Filter</strong> (e.g., cut-off at 300 Hz).</li> <li><strong>Analogy:</strong> The roar of the crowd in a stadium. You hear the collective intensity and mood (cheering vs. booing), but not individual conversations.</li> </ul> <h4 id="1-brain-rhythms-the-radio-analogy">1. Brain Rhythms: The Radio Analogy</h4> <p>Within the LFP signal, distinct cognitive processes utilize different frequency ranges. This is analogous to a <strong>radio</strong>: multiple stations broadcast simultaneously, but you can isolate specific information (news, jazz, rock) by tuning into specific frequency bands.</p> <p>These bands are not arbitrary; they result from specific neurobiological mechanisms (such as synaptic decay times and signal transmission delays). Note that the bands increase <strong>logarithmically</strong> in width.</p> <table> <thead> <tr> <th style="text-align: left">Band</th> <th style="text-align: left">Frequency</th> <th style="text-align: left">Associated States (General)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Delta</strong></td> <td style="text-align: left">2 – 4 Hz</td> <td style="text-align: left">Deep sleep, anesthesia.</td> </tr> <tr> <td style="text-align: left"><strong>Theta</strong></td> <td style="text-align: left">4 – 8 Hz</td> <td style="text-align: left">Memory processing, navigation (Hippocampus), drowsiness.</td> </tr> <tr> <td style="text-align: left"><strong>Alpha</strong></td> <td style="text-align: left">8 – 12 Hz</td> <td style="text-align: left">Visual idling, inhibition of irrelevant regions.</td> </tr> <tr> <td style="text-align: left"><strong>Beta</strong></td> <td style="text-align: left">15 – 30 Hz</td> <td style="text-align: left">Motor control, maintenance of cognitive state.</td> </tr> <tr> <td style="text-align: left"><strong>Lower Gamma</strong></td> <td style="text-align: left">30 – 80 Hz</td> <td style="text-align: left">Feature binding, active information processing.</td> </tr> <tr> <td style="text-align: left"><strong>Upper Gamma</strong></td> <td style="text-align: left">80 – 150 Hz</td> <td style="text-align: left">High-level cognitive functioning.</td> </tr> </tbody> </table> <p><em>Note: Boundaries are not precise (e.g., Theta can vary from 3–9 Hz) and depend on individual differences like age and brain structure.</em></p> <hr/> <h3 id="vi-the-time-frequency-uncertainty-principle">VI. The Time-Frequency Uncertainty Principle</h3> <p>A major challenge in analyzing these rhythms is the trade-off between knowing <strong>when</strong> something happened and <strong>what frequency</strong> it was.</p> <h4 id="1-the-heartbeat-analogy">1. The Heartbeat Analogy</h4> <p>Consider measuring a heartbeat.</p> <ul> <li>To measure a heart rate (Frequency), you need to count beats over time (e.g., 5 seconds). You cannot measure a “rate” in a 1-millisecond instant.</li> <li>If you shorten the window to capture a “transient” change, you lose precision on the exact rate.</li> </ul> <h4 id="2-the-trade-off">2. The Trade-off</h4> <ul> <li><strong>High Temporal Precision:</strong> Allows you to identify exactly when an event occurred, but makes it difficult to distinguish 20 Hz from 20.5 Hz.</li> <li><strong>High Frequency Precision:</strong> Allows you to distinguish exact frequencies, but smears the event over time (you can’t tell if it started at 200ms or 230ms).</li> </ul> <h4 id="3-interpretation-warning">3. Interpretation Warning</h4> <p>When a paper reports “Activity at 15 Hz at 346 ms,” it is a statistical abstraction.</p> <ul> <li><strong>“15 Hz”</strong> actually means a weighted range where 15 Hz contributed maximally.</li> <li><strong>“346 ms”</strong> actually means a weighted sum of activity from preceding and succeeding time points.</li> </ul>]]></content><author><name></name></author><category term="research"/><category term="computational"/><category term="neuroscience"/><summary type="html"><![CDATA[Spikes, LFPs, and frequency bands]]></summary></entry><entry><title type="html">Fundamentals of Phase Locking</title><link href="https://jong-min-moon.github.io/blog/2021/phase-locking/" rel="alternate" type="text/html" title="Fundamentals of Phase Locking"/><published>2021-11-22T00:00:00+00:00</published><updated>2021-11-22T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2021/phase-locking</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2021/phase-locking/"><![CDATA[<h3 id="i-the-core-concept-consistency-of-angles">I. The Core Concept: Consistency of Angles</h3> <p>In neuroscience, <em>phase locking</em> mathematically means <strong>consistency of angles</strong>. You collect a sequence of phase angles (0° to 360°) and ask:</p> <blockquote> <p>Are these angles random (uniform), or do they cluster around a specific direction?</p> </blockquote> <p>However, the term is used in two very different experimental contexts depending on <strong>what</strong> is locking to <strong>what</strong>.</p> <hr/> <h3 id="ii-type-1-spike-field-phase-locking-internal-sync">II. Type 1: Spike-Field Phase Locking (Internal Sync)</h3> <blockquote> <p>“Phase locking refers to the tendency of a neuron to fire action potentials at particular phases of an ongoing periodic sound waveform, such as the sinusoidal waveforms that are typically used in physiological studies of the auditory system”</p> <p>— <em>Encyclopedia of Neuroscience, 2009</em></p> </blockquote> <ul> <li><strong>Synonyms:</strong> Spike-field coherence , spike-phase coupling.</li> <li><strong>What is locking?</strong> A single neuron’s firing (spike train).</li> <li><strong>What is it locking to?</strong> An internal brain rhythm (LFP/Theta Wave).</li> <li><strong>The Reference Frame:</strong> The brain’s clock (the cycle of the wave itself).</li> <li><strong>The Question:</strong> <em>“Does this neuron fire in sync with the background beat of the hippocampus?”</em></li> </ul> <p><strong>Mechanism:</strong> The LFP oscillation acts as a “conductor.” When the voltage is at a specific phase (e.g., the trough), the membrane potential of the neuron is pushed closer to threshold, making it more likely to fire.</p> <ul> <li><strong>Result:</strong> Spikes cluster at the “preferred phase” of the oscillation.</li> <li><strong>Meaning:</strong> This proves <strong>Network Connectivity</strong>. It confirms the single neuron is physically “plugged in” to the local population dynamics.</li> </ul> <hr/> <h3 id="iii-type-2-inter-trial-phase-locking-external-sync">III. Type 2: Inter-Trial Phase Locking (External Sync)</h3> <blockquote> <p>“Activity is phase-locked when its phase is the same or very similar on each trial”</p> <p>— <em>Cohen, M. X. Analyzing Neural Time Series Data: Theory and Practice, Chapter 3</em></p> </blockquote> <p><strong>Context:</strong> EEG/ERP Analysis (Cohen, Chapter 2). <strong>Synonyms:</strong> Inter-Trial Phase Clustering (ITPC), Phase-Locking Value (PLV).</p> <ul> <li><strong>What is locking?</strong> A macroscopic voltage wave (<strong>EEG/LFP</strong>).</li> <li><strong>What is it locking to?</strong> An <strong>External Event</strong> (Stimulus Onset / $t=0$).</li> <li><strong>The Reference Frame:</strong> The Experimenter’s Clock.</li> <li><strong>The Question:</strong> <em>“Does the brain start oscillating with the exact same phase every time I show the picture?”</em></li> </ul> <p><strong>Mechanism:</strong> This distinction is used to separate <strong>Evoked</strong> from <strong>Induced</strong> activity.</p> <ol> <li><strong>Phase-Locked (Evoked):</strong> The stimulus “resets” the phase of the oscillation. On Trial 1, 2, and 3, the wave starts with a <em>peak</em> at 100ms. <ul> <li><em>Result:</em> Creates an <strong>ERP</strong> (Event-Related Potential).</li> </ul> </li> <li><strong>Non-Phase-Locked (Induced):</strong> The stimulus increases the <em>power</em> of the oscillation, but the phase is random. On Trial 1 it’s a peak; on Trial 2 it’s a trough. <ul> <li><em>Result:</em> Invisible in ERPs (cancels out). Visible in Time-Frequency Power.</li> </ul> </li> </ol> <hr/> <h3 id="iv-comparison-the-musician-vs-the-sprinter">IV. Comparison: The Musician vs. The Sprinter</h3> <p>The best way to remember the difference is through analogy:</p> <table> <thead> <tr> <th style="text-align: left">Feature</th> <th style="text-align: left"><strong>Type 1: Spike-Field (Internal)</strong></th> <th style="text-align: left"><strong>Type 2: Inter-Trial (External)</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>The Analogy</strong></td> <td style="text-align: left"><strong>The Jazz Musician</strong></td> <td style="text-align: left"><strong>The Sprinter</strong></td> </tr> <tr> <td style="text-align: left"><strong>Description</strong></td> <td style="text-align: left">A drummer who always hits the snare drum exactly on the <strong>3rd beat</strong> of the song.</td> <td style="text-align: left">A runner who always launches exactly <strong>100ms</strong> after the starting gun fires.</td> </tr> <tr> <td style="text-align: left"><strong>Synchronization</strong></td> <td style="text-align: left">Syncs to the <strong>Rhythm</strong> (Continuous).</td> <td style="text-align: left">Syncs to the <strong>Trigger</strong> (Discrete).</td> </tr> <tr> <td style="text-align: left"><strong>Data Source</strong></td> <td style="text-align: left">Correlation between <strong>Spikes</strong> and <strong>LFP</strong>.</td> <td style="text-align: left">Correlation between <strong>Trial 1</strong>, <strong>Trial 2</strong>, <strong>Trial 3</strong>…</td> </tr> <tr> <td style="text-align: left"><strong>Scientific Value</strong></td> <td style="text-align: left">Validates <strong>Probe Location</strong> and <strong>Network Membership</strong>.</td> <td style="text-align: left">Validates <strong>Stimulus Determinism</strong> and <strong>Evoked Potentials</strong>.</td> </tr> </tbody> </table> <hr/> <h3 id="v-the-mathematical-unity">V. The Mathematical Unity</h3> <p>Despite the conceptual difference, the statistical test used is often identical (The Rayleigh Test).</p> <ol> <li><strong>Extract Angles:</strong> <ul> <li><em>Type 1:</em> Collect the LFP phase angle at every spike timestamp ($\phi_{spike}$).</li> <li><em>Type 2:</em> Collect the EEG phase angle at time $t$ across all trials ($\phi_{trial}$).</li> </ul> </li> <li><strong>Vector Sum:</strong> Treat each angle as a vector on a unit circle.</li> <li><strong>Calculate Length:</strong> <ul> <li>If the vector length $\approx 0$, the activity is <strong>Random</strong> (Unlocked).</li> <li>If the vector length $\approx 1$, the activity is <strong>Clustered</strong> (Locked).</li> </ul> </li> </ol> <blockquote> <p><strong>Summary:</strong></p> <ul> <li><strong>Spike-Field Locking</strong> tells you about the <strong>Brain’s Internal Wiring</strong> (Spike $\leftrightarrow$ Wave).</li> <li><strong>Inter-Trial Locking</strong> tells you about the <strong>Brain’s Reaction to the World</strong> (Stimulus $\leftrightarrow$ Wave).</li> </ul> </blockquote>]]></content><author><name></name></author><category term="research,"/><category term="statistics"/><summary type="html"><![CDATA[Distinguishing Spike-Field Locking from Inter-Trial Phase Locking]]></summary></entry><entry><title type="html">Fundamentals of the Filter-Hilbert Method</title><link href="https://jong-min-moon.github.io/blog/2021/hilbert/" rel="alternate" type="text/html" title="Fundamentals of the Filter-Hilbert Method"/><published>2021-10-25T00:00:00+00:00</published><updated>2021-10-25T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2021/hilbert</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2021/hilbert/"><![CDATA[<h3 id="i-the-core-distinction-decoupling">I. The Core Distinction: Decoupling</h3> <p>The fundamental difference between Wavelet Convolution and the Filter-Hilbert method lies in how they handle the two primary tasks of time-frequency analysis: <strong>Filtering</strong> (isolating frequencies) and <strong>Analytic Representation</strong> (extracting power/phase).</p> <ul> <li><strong>Wavelet Convolution:</strong> Performs both tasks simultaneously. A complex Morlet wavelet is, by definition, a bandpass filter <em>and</em> an analytic signal combined.</li> <li><strong>Filter-Hilbert Method:</strong> Decouples these steps. <ol> <li><strong>Step 1 (Filtering):</strong> Apply a bandpass filter to isolate the frequency of interest.</li> <li><strong>Step 2 (Hilbert):</strong> Apply the Hilbert transform to extract the analytic signal (complex power/phase information).</li> </ol> </li> </ul> <p><strong>The Main Advantage:</strong> By decoupling these steps, you gain significantly more control over the <strong>frequency characteristics</strong> of the filter.</p> <h3 id="ii-the-hilbert-transform">II. The Hilbert Transform</h3> <p>The Hilbert transform is a mathematical operation used to extract complex information from a real-valued signal.</p> <h4 id="1-purpose">1. Purpose</h4> <p>It converts a real-valued signal (which only has cosine components, $M\cos(2\pi ft)$) into an analytic (complex) signal by generating and adding the <strong>imaginary part</strong>.</p> <ul> <li><strong>Real Part:</strong> Original signal ($M\cos(2\pi ft)$).</li> <li><strong>Imaginary Part:</strong> Phase quadrature component ($iM\sin(2\pi ft)$).</li> </ul> <h4 id="2-implementation-the-frequency-domain-trick">2. Implementation (The Frequency Domain Trick)</h4> <p>In practice (like in MATLAB’s <code class="language-plaintext highlighter-rouge">hilbert</code> function), we do not compute this by integrating in the time domain. Instead, we manipulate the Fourier spectrum:</p> <ol> <li><strong>FFT:</strong> Convert signal to the frequency domain.</li> <li><strong>Rotate:</strong> Double the <strong>positive-frequency</strong> coefficients and set the <strong>negative-frequency</strong> coefficients to <strong>zero</strong>.</li> <li><strong>IFFT:</strong> Convert back to the time domain.</li> </ol> <h3 id="iii-the-advantage-filter-control-gaussian-vs-plateau">III. The Advantage: Filter Control (Gaussian vs. Plateau)</h3> <p>The primary limitation of Morlet wavelets is that their frequency shape is <strong>always Gaussian</strong>. You cannot change this shape; you can only change its width (via the number of cycles).</p> <p>The Filter-Hilbert method allows you to design filters with custom shapes:</p> <ul> <li><strong>Plateau Shapes:</strong> Unlike the Gaussian peak of a wavelet, a bandpass filter can have a flat top (“plateau”).</li> <li><strong>Benefit:</strong> This offers better <strong>frequency specificity</strong>. You can define a hard boundary for the frequencies you want to keep versus those you want to remove, resulting in time-frequency plots that are smoother along the frequency axis.</li> </ul> <h3 id="iv-filter-types">IV. Filter Types</h3> <p>Using the Filter-Hilbert method, you can apply four standard types of frequency filters before extracting the analytic signal:</p> <ul> <li><strong>Bandpass:</strong> Keep activity <em>between</em> two frequencies (most useful for time-frequency decomposition).</li> <li><strong>Band-stop:</strong> Remove activity <em>between</em> two frequencies (notch filter).</li> <li><strong>High-pass:</strong> Retain frequencies <em>above</em> a cutoff.</li> <li><strong>Low-pass:</strong> Retain frequencies <em>below</em> a cutoff.</li> </ul> <p><strong>Summary:</strong> While wavelet convolution is mathematically elegant (one step), the Filter-Hilbert method is practically powerful because it allows you to shape your frequency isolation (e.g., using a plateau filter) before extracting the power and phase.</p> <h3 id="v-comparison-with-wavelet-convolution">V. Comparison with Wavelet Convolution</h3> <h4 id="1-smoothness-in-the-frequency-domain">1. Smoothness in the Frequency Domain</h4> <p>While the Filter-Hilbert method allows for “plateau” shaped filters (which have sharp boundaries), this sharpness can sometimes be a disadvantage.</p> <ul> <li><strong>The Wavelet Advantage:</strong> Because Morlet wavelets always have a Gaussian shape in the frequency domain, they produce <strong>time-frequency plots that appear smoother</strong> along the frequency axis compared to the Filter-Hilbert method.</li> </ul> <h4 id="2-artifact-minimization-time-domain">2. Artifact Minimization (Time Domain)</h4> <p>The shape of the window matters significantly for preventing artifacts in your data.</p> <ul> <li><strong>No Sharp Edges:</strong> Morlet wavelets use a Gaussian taper. Unlike “box-car” (square) filters, Gaussian windows have <strong>no sharp edges</strong>.</li> <li><strong>Consequence:</strong> Sharp edges in a filter kernel can produce “edge artifacts” or ripples in the time domain. The smooth Gaussian taper of the Morlet wavelet prevents these artifacts, ensuring that the influence of surrounding time points is dampened smoothly.</li> </ul> <h4 id="3-the-optimal-precision-trade-off">3. The Optimal Precision Trade-off</h4> <p>The Gaussian shape of the Morlet wavelet is not arbitrary; it is mathematically optimized.</p> <ul> <li><strong>Control:</strong> The Gaussian window allows you to precisely control the trade-off between <strong>temporal precision</strong> and <strong>frequency precision</strong>.</li> <li><strong>Stationarity:</strong> The stationarity assumption for wavelets is limited to the brief window where the wavelet is non-zero. This is a much safer assumption for neural data (which is often stationary for only hundreds of milliseconds) compared to methods that require longer windows.</li> </ul> <h4 id="4-summary-when-to-use-which">4. Summary: When to Use Which?</h4> <ul> <li><strong>Use Filter-Hilbert:</strong> When you need <strong>exact control</strong> over the filter shape (e.g., a flat “plateau” to treat all frequencies in a band exactly equally).</li> <li><strong>Use Morlet Wavelets:</strong> When you prioritize <strong>smoothness</strong> in your results, artifact minimization, and a mathematically robust trade-off between time and frequency precision.</li> </ul>]]></content><author><name></name></author><category term="research,"/><category term="computational"/><category term="neuroscience"/><summary type="html"><![CDATA[Decoupling filtering from the analytic signal for better frequency control]]></summary></entry></feed>
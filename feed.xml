<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jong-min-moon.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jong-min-moon.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-07T05:24:55+00:00</updated><id>https://jong-min-moon.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Jongmin Mun. </subtitle><entry><title type="html">Functional Connectivity &amp;amp; Heterogeneous Treatment Effects</title><link href="https://jong-min-moon.github.io/blog/2024/functional-connectivity/" rel="alternate" type="text/html" title="Functional Connectivity &amp;amp; Heterogeneous Treatment Effects"/><published>2024-12-15T00:00:00+00:00</published><updated>2024-12-15T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2024/functional-connectivity</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2024/functional-connectivity/"><![CDATA[<p>We model the brain as a functional network where nodes represent regions of interest (ROIs) and edges represent the correlation of their time series (functional connectivity). To understand how ASD affects this connectivity differently in males and females, we estimate <strong>Heterogeneous Treatment Effects (HTE)</strong>. Our framework allows us to identify specific connections that are altered by the condition in a sex-specific manner.</p>]]></content><author><name></name></author><category term="research"/><summary type="html"><![CDATA[Mapping causal networks in the brain.]]></summary></entry><entry><title type="html">Results: Higher-Order Cognitive Control</title><link href="https://jong-min-moon.github.io/blog/2024/hte-results/" rel="alternate" type="text/html" title="Results: Higher-Order Cognitive Control"/><published>2024-04-10T00:00:00+00:00</published><updated>2024-04-10T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2024/hte-results</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2024/hte-results/"><![CDATA[<p>Our results indicate that the sex differences in ASD are not random. The identified Heterogeneous Treatment Effects (HTEs) are strongly clustered in regions associated with <strong>higher-order cognitive control functions</strong>, rather than ensuring lower-level sensory processing areas. This suggests that the manifestation of ASD may diverge between sexes particularly in how complex information is integrated and regulated.</p>]]></content><author><name></name></author><category term="research"/><summary type="html"><![CDATA[Identifying key brain regions with sex-specific alterations.]]></summary></entry><entry><title type="html">Wavelet Basis &amp;amp; Function Spaces: Besov vs. Hölder</title><link href="https://jong-min-moon.github.io/blog/2022/DWT/" rel="alternate" type="text/html" title="Wavelet Basis &amp;amp; Function Spaces: Besov vs. Hölder"/><published>2022-10-31T00:00:00+00:00</published><updated>2022-10-31T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2022/DWT</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2022/DWT/"><![CDATA[<h3 id="i-context-signal-processing-vs-statistical-estimation">I. Context: Signal Processing vs. Statistical Estimation</h3> <p>Before diving into Besov spaces, it is crucial to understand the fundamental shift from <strong>Convolution</strong> (used in neuroscience/signal processing) to <strong>Basis Expansion</strong> (used in statistics).</p> <table> <thead> <tr> <th style="text-align: left">Feature</th> <th style="text-align: left"><strong>Wavelet Convolution (CWT)</strong></th> <th style="text-align: left"><strong>Wavelet Basis (DWT)</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Goal</strong></td> <td style="text-align: left">Phase, Power Extraction &amp; Visualization</td> <td style="text-align: left">Estimation, Compression, Denoising</td> </tr> <tr> <td style="text-align: left"><strong>Method</strong></td> <td style="text-align: left">Sliding window (Redundant)</td> <td style="text-align: left">Tiling / Grid (Orthogonal)</td> </tr> <tr> <td style="text-align: left"><strong>Structure</strong></td> <td style="text-align: left">Smooth, overlapping coefficients</td> <td style="text-align: left">Sparse, independent coefficients</td> </tr> <tr> <td style="text-align: left"><strong>Wavelet</strong></td> <td style="text-align: left">Complex Morlet (Smooth)</td> <td style="text-align: left">Haar, Daubechies (Compact/Step)</td> </tr> </tbody> </table> <p>In the statistical context (e.g., nonparametric regression), we prioritize <strong>efficiency</strong> and <strong>sparsity</strong>. We want to reconstruct a function $f(t)$ using the fewest number of coefficients possible.</p> <hr/> <h3 id="ii-the-haar-wavelet-basis">II. The Haar Wavelet Basis</h3> <p>The text provided focuses on the <strong>Haar Multivariate Wavelet Basis</strong>. This system creates a hierarchical representation of a function using step-like “building blocks.”</p> <h4 id="1-the-structure-setting-j0">1. The Structure (Setting $J=0$)</h4> <p>The analysis starts at the coarsest level ($J=0$), dividing the function into two categories:</p> <ul> <li><strong>Scaling Coefficients ($\theta_\phi$):</strong> <ul> <li><strong>Symbol:</strong> $\Phi_0$</li> <li><strong>Role:</strong> Represents the <strong>Global Trend</strong> or average of the function over the domain $[0,1]^d$. The coefficient is simply just the mean of the function.</li> </ul> </li> <li><strong>Wavelet Coefficients ($\theta_\psi$):</strong> <ul> <li><strong>Symbol:</strong> $\Psi_j$ (for levels $j \ge 0$)</li> <li><strong>Role:</strong> Captures <strong>Abrupt Oscillations</strong> and details that deviate from the global trend.</li> <li><strong>Resolution:</strong> As level $j$ increases, the wavelets become narrower and taller, capturing higher-frequency details.</li> </ul> </li> </ul> <h4 id="2-why-haar">2. Why Haar?</h4> <p>The Haar basis is chosen because projecting a density onto this basis is mathematically equivalent to <strong>Equal-Sized Binning</strong> or histogramization. The resolution level decides the bin width and the coefficients are the bin heights. This allows researchers to link abstract function space theory directly to the discretization error inherent in statistical testing.</p> <hr/> <h3 id="iii-the-besov-ball-a-budgeting-game">III. The Besov Ball: A “Budgeting Game”</h3> <p>How do we define if a function is “smooth”? The <strong>Besov Norm</strong> ($|||f|||_{s,2,q}$) measures smoothness by calculating the “cost” of building the function using these wavelet blocks.</p> \[|||f|||_{s,2,q} := \left[ \sum_{j=0}^{\infty} \underbrace{2^{jsq}}_{\text{Price Tag}} \underbrace{\left( \sum_{\psi \in \Psi_j} |\theta_{\psi}(f)|^2 \right)^{q/2}}_{\text{Energy at Level } j} \right]^{1/q}\] <h4 id="1-the-price-tag-2js">1. The “Price Tag” ($2^{js}$)</h4> <p>This term acts as a weighted penalty.</p> <ul> <li><strong>Low $j$ (Coarse levels):</strong> Cheap. You can use these blocks freely.</li> <li><strong>High $j$ (Fine details):</strong> Expensive. The cost grows exponentially ($2^{js}$).</li> </ul> <h4 id="2-the-rule-of-the-besov-ball">2. The Rule of the Besov Ball</h4> <p>To stay inside the Besov Ball (i.e., to have a finite norm), you must be “thrifty.” You are allowed to use high-frequency wavelets (high $j$), but you must use them <strong>sparingly</strong> (Sparsity).</p> <p>This definition allows the Besov space to accommodate <strong>Spatially Inhomogeneous</strong> functions—functions that are smooth in most places but have occasional sharp spikes or jumps.</p> <hr/> <h3 id="iv-comparison-besov-bs_pq-vs-hölder-cs">IV. Comparison: Besov ($B^s_{p,q}$) vs. Hölder ($C^s$)</h3> <p>This is the critical distinction for statistical modeling.</p> <h4 id="1-the-hölder-class-cs">1. The Hölder Class ($C^s$)</h4> <ul> <li><strong>Philosophy:</strong> <strong>Uniform Regularity</strong> (Worst-Case).</li> <li><strong>The Rule:</strong> The function must be smooth <strong>everywhere</strong>.</li> <li><strong>Sensitivity:</strong> If the function has a single “bad” point (a sharp corner, a jump, a spike) <em>anywhere</em>, the entire function is rejected.</li> <li><strong>Norm Analogy:</strong> Based on $L^\infty$ (Maximum error).</li> </ul> <h4 id="2-the-besov-space-bs_pq">2. The Besov Space ($B^s_{p,q}$)</h4> <ul> <li><strong>Philosophy:</strong> <strong>Average Regularity</strong>.</li> <li><strong>The Rule:</strong> The function must be smooth <strong>on average</strong>.</li> <li><strong>Flexibility:</strong> It tolerates local irregularities (like jumps or spikes) as long as they are spatially sparse. The “cost” of one bad point is averaged out over the smooth regions.</li> <li><strong>Norm Analogy:</strong> Based on $L^p$ (Integrated error).</li> </ul> <h4 id="3-specific-different-functions">3. Specific “Different Functions”</h4> <p>Because of this flexibility, Besov spaces contain functions that are banned from Hölder classes:</p> <table> <thead> <tr> <th style="text-align: left">Function Type</th> <th style="text-align: left">Description</th> <th style="text-align: left">Hölder Class ($C^s$)</th> <th style="text-align: left">Besov Space ($B^s_{p,q}$)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Step Function</strong></td> <td style="text-align: left">Flat $\rightarrow$ Jump $\rightarrow$ Flat</td> <td style="text-align: left"><strong>REJECTED</strong> (Infinite derivative at jump)</td> <td style="text-align: left"><strong>ACCEPTED</strong> (Jump requires only sparse coefficients)</td> </tr> <tr> <td style="text-align: left"><strong>Local Spike</strong></td> <td style="text-align: left">Smooth line with one sharp burst</td> <td style="text-align: left"><strong>REJECTED</strong> (Fails at the burst location)</td> <td style="text-align: left"><strong>ACCEPTED</strong> (Averaged out by smooth regions)</td> </tr> </tbody> </table> <h3 id="v-summary">V. Summary</h3> <p>By selecting the <strong>Haar Basis</strong> and <strong>Besov Spaces</strong>, one explicitly chooses a framework that can model <strong>discontinuities</strong> and <strong>abrupt changes</strong>. Unlike Fourier/Hölder methods, which assume data is uniformly smooth (like a sine wave), Besov/Haar methods assume data may have sharp edges and handle them robustly without “blowing up” the error metric.</p>]]></content><author><name></name></author><category term="research"/><category term="statistics"/><summary type="html"><![CDATA[Understanding Nonparametric Regression, Haar Wavelets, and Smoothness Norms]]></summary></entry><entry><title type="html">Fundamentals of Neural Signal Decomposition</title><link href="https://jong-min-moon.github.io/blog/2021/bands/" rel="alternate" type="text/html" title="Fundamentals of Neural Signal Decomposition"/><published>2021-11-22T00:00:00+00:00</published><updated>2021-11-22T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2021/bands</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2021/bands/"><![CDATA[<h3 id="i-neural-recording-is-a-mixture">I. Neural recording is a mixture</h3> <p>When a microelectrode is inserted into brain tissue, the voltage trace it records contains multiple frequencies simultaneously. These components can be separated through signal-processing techniques. This concept is analogous to a radio: while many stations broadcast simultaneously, they can be isolated by tuning into the specific frequency bands in which they transmit information.</p> <h3 id="ii-signal-separation-individual-vs-population-activity">II. Signal Separation: Individual vs. Population Activity</h3> <p>Unlike EEG, which captures distant signals from the scalp, a microelectrode inserted directly into brain tissue sits in close proximity to neurons. As a result, the recorded signal represents a mixture of activity at two distinct scales: single-unit spikes from individual neurons and aggregate population activity.</p> <p>To make sense of this, consider the electrode as a radio receiver. Just as a radio picks up a chaotic mix of signals where each station occupies a distinct frequency band, the raw neural recording contains multiple physiological phenomena operating at distinct frequencies. By applying digital filters, we first separate this raw trace into two fundamental components:</p> <ol> <li><strong>Low-Frequency (&lt;300 Hz):</strong> Represents <strong>LFP</strong> (Local Field Potential).</li> <li><strong>High-Frequency (&gt;300 Hz):</strong> Represents <strong>Spikes</strong> (Single-Neuron Action Potentials).</li> </ol> <p>The following table highlights the fundamental differences between these two signal types.</p> <table> <thead> <tr> <th style="text-align: left">Feature</th> <th style="text-align: left"><strong>Local Field Potential (LFP)</strong></th> <th style="text-align: left"><strong>Single-Neuron Spikes</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>What it Measures</strong></td> <td style="text-align: left">Summed electrical activity from <em>many</em> nearby neurons.</td> <td style="text-align: left">Rapid action potentials from <em>individual</em> neurons.</td> </tr> <tr> <td style="text-align: left"><strong>Physiological Origin</strong></td> <td style="text-align: left"><strong>Synaptic Inputs:</strong> Current flowing into/out of dendrites (subthreshold activity).</td> <td style="text-align: left"><strong>Neuronal Outputs:</strong> Voltage-gated sodium channel firing (action potentials).</td> </tr> <tr> <td style="text-align: left"><strong>Frequency Range</strong></td> <td style="text-align: left"><strong>Slow:</strong> 1 – 300 Hz</td> <td style="text-align: left"><strong>Fast:</strong> 300 – 5000 Hz (Events ≈ 1 ms)</td> </tr> <tr> <td style="text-align: left"><strong>Spatial Range</strong></td> <td style="text-align: left"><strong>Wide:</strong> ~0.5 – 1 mm radius (hundreds of neurons).</td> <td style="text-align: left"><strong>Narrow:</strong> ~50 – 100 $\mu$m radius (very close to tip).</td> </tr> <tr> <td style="text-align: left"><strong>Information Content</strong></td> <td style="text-align: left">Network-level dynamics, oscillations (Theta, Gamma), population coordination.</td> <td style="text-align: left">Precise firing rates, spike timing, stimulus selectivity, neural coding.</td> </tr> <tr> <td style="text-align: left"><strong>Analysis Methods</strong></td> <td style="text-align: left">Power spectra, Coherence, Phase synchronization.</td> <td style="text-align: left">Spike sorting, Tuning curves, Temporal coding.</td> </tr> </tbody> </table> <hr/> <p><strong>A Note on Frequency Scales:</strong> To intuitively grasp these timescales, consider that even 50 Hz is perceived as continuous. A four-cylinder car engine running at 3,000 RPM cycles at exactly 50 Hz (Gamma band). At this speed, you hear a continuous “hum” rather than discrete engine strokes. The 300 Hz cutoff is six times faster than this engine hum, effectively separating the slow, rhythmic background (LFP) from the extremely rapid, discrete events of neuronal firing (Spikes).</p> <h3 id="iii-high-frequency-component-spikes">III. High-Frequency Component (Spikes)</h3> <ul> <li><strong>Source:</strong> When a specific neuron close to the electrode fires, it creates a brief, high-amplitude voltage deflection.</li> <li><strong>Processing:</strong> Extracted using a <strong>High-Pass Filter</strong> (e.g., cut-off at 300 Hz).</li> <li><strong>Analogy:</strong> The voice of the person sitting right next to you. You hear their specific words and timing distinct from the background roar.</li> </ul> <h3 id="iv-low-frequency-component-lfp">IV. Low-Frequency Component (LFP)</h3> <ul> <li><strong>Source:</strong> As neurons receive input, currents flow into dendrites. These currents overlap and add up spatially, producing a smooth, slow-changing signal.</li> <li><strong>Processing:</strong> Extracted using a <strong>Low-Pass Filter</strong> (e.g., cut-off at 300 Hz).</li> <li><strong>Analogy:</strong> The roar of the crowd in a stadium. You hear the collective intensity and mood (cheering vs. booing), but not individual conversations.</li> </ul> <h4 id="1-brain-rhythms-the-radio-analogy">1. Brain Rhythms: The Radio Analogy</h4> <p>Within the LFP signal, distinct cognitive processes utilize different frequency ranges. This is analogous to a <strong>radio</strong>: multiple stations broadcast simultaneously, but you can isolate specific information (news, jazz, rock) by tuning into specific frequency bands.</p> <p>These bands are not arbitrary; they result from specific neurobiological mechanisms (such as synaptic decay times and signal transmission delays). Note that the bands increase <strong>logarithmically</strong> in width.</p> <table> <thead> <tr> <th style="text-align: left">Band</th> <th style="text-align: left">Frequency</th> <th style="text-align: left">Associated States (General)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Delta</strong></td> <td style="text-align: left">2 – 4 Hz</td> <td style="text-align: left">Deep sleep, anesthesia.</td> </tr> <tr> <td style="text-align: left"><strong>Theta</strong></td> <td style="text-align: left">4 – 8 Hz</td> <td style="text-align: left">Memory processing, navigation (Hippocampus), drowsiness.</td> </tr> <tr> <td style="text-align: left"><strong>Alpha</strong></td> <td style="text-align: left">8 – 12 Hz</td> <td style="text-align: left">Visual idling, inhibition of irrelevant regions.</td> </tr> <tr> <td style="text-align: left"><strong>Beta</strong></td> <td style="text-align: left">15 – 30 Hz</td> <td style="text-align: left">Motor control, maintenance of cognitive state.</td> </tr> <tr> <td style="text-align: left"><strong>Lower Gamma</strong></td> <td style="text-align: left">30 – 80 Hz</td> <td style="text-align: left">Feature binding, active information processing.</td> </tr> <tr> <td style="text-align: left"><strong>Upper Gamma</strong></td> <td style="text-align: left">80 – 150 Hz</td> <td style="text-align: left">High-level cognitive functioning.</td> </tr> </tbody> </table> <p><em>Note: Boundaries are not precise (e.g., Theta can vary from 3–9 Hz) and depend on individual differences like age and brain structure.</em></p> <hr/> <h3 id="vi-the-time-frequency-uncertainty-principle">VI. The Time-Frequency Uncertainty Principle</h3> <p>A major challenge in analyzing these rhythms is the trade-off between knowing <strong>when</strong> something happened and <strong>what frequency</strong> it was.</p> <h4 id="1-the-heartbeat-analogy">1. The Heartbeat Analogy</h4> <p>Consider measuring a heartbeat.</p> <ul> <li>To measure a heart rate (Frequency), you need to count beats over time (e.g., 5 seconds). You cannot measure a “rate” in a 1-millisecond instant.</li> <li>If you shorten the window to capture a “transient” change, you lose precision on the exact rate.</li> </ul> <h4 id="2-the-trade-off">2. The Trade-off</h4> <ul> <li><strong>High Temporal Precision:</strong> Allows you to identify exactly when an event occurred, but makes it difficult to distinguish 20 Hz from 20.5 Hz.</li> <li><strong>High Frequency Precision:</strong> Allows you to distinguish exact frequencies, but smears the event over time (you can’t tell if it started at 200ms or 230ms).</li> </ul> <h4 id="3-interpretation-warning">3. Interpretation Warning</h4> <p>When a paper reports “Activity at 15 Hz at 346 ms,” it is a statistical abstraction.</p> <ul> <li><strong>“15 Hz”</strong> actually means a weighted range where 15 Hz contributed maximally.</li> <li><strong>“346 ms”</strong> actually means a weighted sum of activity from preceding and succeeding time points.</li> </ul>]]></content><author><name></name></author><category term="research"/><category term="computational"/><category term="neuroscience"/><summary type="html"><![CDATA[Spikes, LFPs, and frequency bands]]></summary></entry><entry><title type="html">Maze</title><link href="https://jong-min-moon.github.io/blog/2021/maze/" rel="alternate" type="text/html" title="Maze"/><published>2021-11-22T00:00:00+00:00</published><updated>2021-11-22T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2021/maze</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2021/maze/"><![CDATA[<h2 id="study-objective">Study Objective</h2> <p>We aim to demonstrate the recording efficacy of the neural interface system developed by Park et al. (2024) in detecting individual and population level neural activities in a freely behaving mouse model.</p> <h2 id="target-brain-regions">Target Brain Regions</h2> <p>We implant neural brob in two distnct, functionally specialized brain regions to correlate neural activity with specific behavioral states.</p> <table> <thead> <tr> <th style="text-align: left">Target Region</th> <th style="text-align: left">Anatomical Location</th> <th style="text-align: left">Functional Association</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>CA1</strong></td> <td style="text-align: left">Hippocampus (Cornu Ammonis 1)</td> <td style="text-align: left">Navigation, spatial memory, and active locomotion.</td> </tr> <tr> <td style="text-align: left"><strong>L6</strong></td> <td style="text-align: left">Primary Visual Cortex (Layer 6)</td> <td style="text-align: left">Visual processing and head-rotation motion.</td> </tr> </tbody> </table> <div class="fake-img l-body"> <img src="/assets/img/mouse_brain_region_LA6_C1.png" alt="Mouse Brain Regions"/> <div class="caption"> Target brain regions: CA1 (green) and L6 (blue) </div> </div> <h2 id="mouse-task">Mouse task</h2> <p>Freely moving mice were placed at the start of the long arm of a T-shaped maze. As they traversed the track during the <strong>active locomotion</strong> phase, we monitored neural activity, specifically anticipating the activation of place units in the <strong>hippocampus (CA1)</strong>. When the mice reached the intersection, they transitioned to a <strong>stopping and orienting</strong> phase, rotating their heads to scan for visual patterns marking the exit path. This visual discrimination task was expected to drive neuronal activity in the <strong>primary visual cortex (L6)</strong>. Simultaneously, one neural probe recorded from CA1 and another from L6 to capture these region-specific dynamics.</p> <div class="fake-img l-body"> <img src="/assets/img/t_maze.png" alt="Mouse Task"/> <div class="caption"> Mouse task: T-maze </div> </div> <p>| Task Phase | Behavior | Relevant Brain Region | | :— | :— | :— | | <strong>Task 1: Locomotion</strong> | Running along the long arm | Hippocampus (CA1) | | <strong>Task 2: Orientation</strong> | Stopping and head rotation at intersection | Visual Cortex (L6) | —</p> <h3 id="iii-task-1-active-locomotion-analysis">III. Task 1: Active Locomotion Analysis</h3> <p>We analyzed CA1 neural signals by comparing periods of active running versus stopping. Specifically, we examined the <strong>Local Field Potential (LFP)</strong> by filtering for the <strong>Theta band</strong> and analyzing both the amplitude and phase traces. Additionally, we monitored single-unit activity by calculating spike firing rates. If the neural probe is functioning correctly, we expect to observe significantly higher activity in CA1—manifested as increased LFP theta power and elevated spike rates—during running compared to stopping.</p> <p>The experimental results showed that the neural probe properly works, as expected.</p> <hr/> <h3 id="iv-validation-methodology-phase-locking-analysis">IV. Validation Methodology: Phase-Locking Analysis</h3> <p>To verify that the recorded signals represented genuine neural physiology rather than artifacts or noise, the study performed a <strong>Phase-Locking Analysis (PLA)</strong>.</p> <h4 id="1-the-rationale">1. The Rationale</h4> <p>The implanted probes capture a single raw voltage trace that is separated into two components via filtering:</p> <ul> <li><strong>LFP (1–300 Hz):</strong> Reflects collective network activity.</li> <li><strong>Spikes (500–3000 Hz):</strong> Reflects individual neuronal output.</li> </ul> <p>Since both signals originate from the same biological source, they should be temporally correlated. If the signals were noise, the spikes would occur randomly relative to the background oscillation.</p> <h4 id="2-the-mechanism-in-context">2. The Mechanism in Context</h4> <ul> <li><strong>Context:</strong> During active running, the hippocampus generates robust <strong>Theta oscillations (6–10 Hz)</strong>.</li> <li><strong>Analysis:</strong> The phase of the Theta oscillation is extracted (via Hilbert transform) at the exact moment each spike occurs.</li> <li><strong>Criterion:</strong> <ul> <li><em>Uniform Distribution (0°–360°):</em> Indicates random firing (No relationship / Potential noise).</li> <li><em>Clustered Distribution:</em> Indicates <strong>Phase Locking</strong>.</li> </ul> </li> </ul> <h4 id="3-finding">3. Finding</h4> <p>The analysis revealed that spikes from CA1 neurons clustered around a <strong>preferred phase</strong> of the LFP theta rhythm. This confirms <strong>significant phase locking</strong>, demonstrating precise temporal coordination between individual neurons and the broader hippocampal network, thereby validating the signal quality of the interface</p>]]></content><author><name></name></author><category term="research"/><category term="computational-neuroscience"/><category term="paper-details"/><category term="real-data-analysis"/><summary type="html"><![CDATA[Time-frequency decomposition using Analytic and Complex Morlet Wavelets]]></summary></entry><entry><title type="html">Fundamentals of the Filter-Hilbert Method</title><link href="https://jong-min-moon.github.io/blog/2021/hilbert/" rel="alternate" type="text/html" title="Fundamentals of the Filter-Hilbert Method"/><published>2021-10-25T00:00:00+00:00</published><updated>2021-10-25T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2021/hilbert</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2021/hilbert/"><![CDATA[<h3 id="i-the-core-distinction-decoupling">I. The Core Distinction: Decoupling</h3> <p>The fundamental difference between Wavelet Convolution and the Filter-Hilbert method lies in how they handle the two primary tasks of time-frequency analysis: <strong>Filtering</strong> (isolating frequencies) and <strong>Analytic Representation</strong> (extracting power/phase).</p> <ul> <li><strong>Wavelet Convolution:</strong> Performs both tasks simultaneously. A complex Morlet wavelet is, by definition, a bandpass filter <em>and</em> an analytic signal combined.</li> <li><strong>Filter-Hilbert Method:</strong> Decouples these steps. <ol> <li><strong>Step 1 (Filtering):</strong> Apply a bandpass filter to isolate the frequency of interest.</li> <li><strong>Step 2 (Hilbert):</strong> Apply the Hilbert transform to extract the analytic signal (complex power/phase information).</li> </ol> </li> </ul> <p><strong>The Main Advantage:</strong> By decoupling these steps, you gain significantly more control over the <strong>frequency characteristics</strong> of the filter.</p> <h3 id="ii-the-hilbert-transform">II. The Hilbert Transform</h3> <p>The Hilbert transform is a mathematical operation used to extract complex information from a real-valued signal.</p> <h4 id="1-purpose">1. Purpose</h4> <p>It converts a real-valued signal (which only has cosine components, $M\cos(2\pi ft)$) into an analytic (complex) signal by generating and adding the <strong>imaginary part</strong>.</p> <ul> <li><strong>Real Part:</strong> Original signal ($M\cos(2\pi ft)$).</li> <li><strong>Imaginary Part:</strong> Phase quadrature component ($iM\sin(2\pi ft)$).</li> </ul> <h4 id="2-implementation-the-frequency-domain-trick">2. Implementation (The Frequency Domain Trick)</h4> <p>In practice (like in MATLAB’s <code class="language-plaintext highlighter-rouge">hilbert</code> function), we do not compute this by integrating in the time domain. Instead, we manipulate the Fourier spectrum:</p> <ol> <li><strong>FFT:</strong> Convert signal to the frequency domain.</li> <li><strong>Rotate:</strong> Double the <strong>positive-frequency</strong> coefficients and set the <strong>negative-frequency</strong> coefficients to <strong>zero</strong>.</li> <li><strong>IFFT:</strong> Convert back to the time domain.</li> </ol> <h3 id="iii-the-advantage-filter-control-gaussian-vs-plateau">III. The Advantage: Filter Control (Gaussian vs. Plateau)</h3> <p>The primary limitation of Morlet wavelets is that their frequency shape is <strong>always Gaussian</strong>. You cannot change this shape; you can only change its width (via the number of cycles).</p> <p>The Filter-Hilbert method allows you to design filters with custom shapes:</p> <ul> <li><strong>Plateau Shapes:</strong> Unlike the Gaussian peak of a wavelet, a bandpass filter can have a flat top (“plateau”).</li> <li><strong>Benefit:</strong> This offers better <strong>frequency specificity</strong>. You can define a hard boundary for the frequencies you want to keep versus those you want to remove, resulting in time-frequency plots that are smoother along the frequency axis.</li> </ul> <h3 id="iv-filter-types">IV. Filter Types</h3> <p>Using the Filter-Hilbert method, you can apply four standard types of frequency filters before extracting the analytic signal:</p> <ul> <li><strong>Bandpass:</strong> Keep activity <em>between</em> two frequencies (most useful for time-frequency decomposition).</li> <li><strong>Band-stop:</strong> Remove activity <em>between</em> two frequencies (notch filter).</li> <li><strong>High-pass:</strong> Retain frequencies <em>above</em> a cutoff.</li> <li><strong>Low-pass:</strong> Retain frequencies <em>below</em> a cutoff.</li> </ul> <p><strong>Summary:</strong> While wavelet convolution is mathematically elegant (one step), the Filter-Hilbert method is practically powerful because it allows you to shape your frequency isolation (e.g., using a plateau filter) before extracting the power and phase.</p> <h3 id="v-comparison-with-wavelet-convolution">V. Comparison with Wavelet Convolution</h3> <h4 id="1-smoothness-in-the-frequency-domain">1. Smoothness in the Frequency Domain</h4> <p>While the Filter-Hilbert method allows for “plateau” shaped filters (which have sharp boundaries), this sharpness can sometimes be a disadvantage.</p> <ul> <li><strong>The Wavelet Advantage:</strong> Because Morlet wavelets always have a Gaussian shape in the frequency domain, they produce <strong>time-frequency plots that appear smoother</strong> along the frequency axis compared to the Filter-Hilbert method.</li> </ul> <h4 id="2-artifact-minimization-time-domain">2. Artifact Minimization (Time Domain)</h4> <p>The shape of the window matters significantly for preventing artifacts in your data.</p> <ul> <li><strong>No Sharp Edges:</strong> Morlet wavelets use a Gaussian taper. Unlike “box-car” (square) filters, Gaussian windows have <strong>no sharp edges</strong>.</li> <li><strong>Consequence:</strong> Sharp edges in a filter kernel can produce “edge artifacts” or ripples in the time domain. The smooth Gaussian taper of the Morlet wavelet prevents these artifacts, ensuring that the influence of surrounding time points is dampened smoothly.</li> </ul> <h4 id="3-the-optimal-precision-trade-off">3. The Optimal Precision Trade-off</h4> <p>The Gaussian shape of the Morlet wavelet is not arbitrary; it is mathematically optimized.</p> <ul> <li><strong>Control:</strong> The Gaussian window allows you to precisely control the trade-off between <strong>temporal precision</strong> and <strong>frequency precision</strong>.</li> <li><strong>Stationarity:</strong> The stationarity assumption for wavelets is limited to the brief window where the wavelet is non-zero. This is a much safer assumption for neural data (which is often stationary for only hundreds of milliseconds) compared to methods that require longer windows.</li> </ul> <h4 id="4-summary-when-to-use-which">4. Summary: When to Use Which?</h4> <ul> <li><strong>Use Filter-Hilbert:</strong> When you need <strong>exact control</strong> over the filter shape (e.g., a flat “plateau” to treat all frequencies in a band exactly equally).</li> <li><strong>Use Morlet Wavelets:</strong> When you prioritize <strong>smoothness</strong> in your results, artifact minimization, and a mathematically robust trade-off between time and frequency precision.</li> </ul>]]></content><author><name></name></author><category term="research,"/><category term="computational"/><category term="neuroscience"/><summary type="html"><![CDATA[Decoupling filtering from the analytic signal for better frequency control]]></summary></entry><entry><title type="html">Fundamentals of Continuous Wavelet Transform</title><link href="https://jong-min-moon.github.io/blog/2021/wavelet/" rel="alternate" type="text/html" title="Fundamentals of Continuous Wavelet Transform"/><published>2021-10-11T00:00:00+00:00</published><updated>2021-10-11T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2021/wavelet</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2021/wavelet/"><![CDATA[<h3 id="i-motivation">I. Motivation</h3> <p>For neural signals, the Fourier transform serves primarily as a computational backbone for convolution rather than a final analytical endpoint. Our goal is to uncover not just the frequency content, but the <strong>time-frequency representation</strong> of the signal. Simply put, for a specific frequency band, we want to extract three traces:</p> <ol> <li>the instantaneous power at every time point,</li> <li>the instantaneous phase at every time point,</li> <li>the bandpass-filtered signal.</li> </ol> <p>This is achieved through continuous wavelet transform (CWT). CWT is performed by convolving the signal with a <strong>Morlet Wavelet</strong> defined by a specific frequency.</p> <p>The <strong>Continuous Wavelet Transform (CWT)</strong> is primarily used for <strong>Time-Frequency Representation (TFR)</strong>, for the analysis of traces. In this context, feature extraction means isolating specific time-frequency characteristics. It’s not the ML sense of feature extraction. However, because CWT utilizes a redundant set of basis functions, the extracted features (trace values over time) are highly correlated. This redundancy makes them less suitable as direct features for many machine learning and statistical models. For regression analysis, statisticians typically prefer the <strong>Discrete Wavelet Transform (DWT)</strong>, which uses a discrete set of scales and translations to provide a more parsimonious and orthogonal signal representation. DWT is explained in <a href="https://jong-min.org/blog/2022/DWT/">this post</a>.</p> <p><strong>The Goal:</strong> We need a method for <strong>Time-Frequency Representation (TFR)</strong> that balances temporal and frequency precision.</p> <h3 id="ii-algorithm-overview">II. Algorithm overview</h3> <ul> <li>Input: time series signal, wavelet parameters (frequency, number of cycles)</li> <li>Output: three time series traces (instantaneous power, instantaneous phase, bandpass-filtered signal)</li> <li> <h2 id="computational-backbone-convolution-continuosly-many-inner-products">Computational backbone: Convolution (continuosly many inner products)</h2> </li> </ul> <h3 id="iii-the-morlet-wavelet">III. The Morlet Wavelet</h3> <p>To localize frequency information in time, we must restrict the sine wave so it does not extend infinitely.</p> <h4 id="1-definition">1. Definition</h4> <p>A Morlet wavelet is created by multiplying a <strong>Sine Wave</strong> by a <strong>Gaussian Window</strong> (taper).</p> <ul> <li><strong>Sine Wave:</strong> Provides the frequency specificity.</li> <li><strong>Gaussian Window:</strong> Localizes the wave in time and dampens the edges to zero to prevent artifacts.</li> </ul> <h4 id="2-the-gaussian-formula">2. The Gaussian Formula</h4> <p>The shape of the taper is defined by:</p> \[GaussWin(t) = a e^{-(t-m)^2 / (2s^2)}\] <ul> <li>\( a \): Amplitude.</li> <li>\( t \): Time.</li> <li>\( m \): Time offset (usually 0).</li> <li>\( s \): Standard deviation (width) of the Gaussian.</li> </ul> <h4 id="3-the-standard-deviation-s">3. The Standard Deviation (s)</h4> <p>The width of the wavelet \( s \) is determined by the frequency \( f \) and the number of cycles \( n \):</p> \[s = \frac{n}{2\pi f}\] <ul> <li>\( n \): <strong>Number of cycles</strong>. This is the critical user-defined parameter that controls the trade-off between temporal and frequency precision.</li> </ul> <hr/> <h3 id="iv-the-limitation-of-real-wavelets">IV. The Limitation of Real Wavelets</h3> <p>A standard “real-valued” Morlet wavelet has a major flaw: the result of convolution depends on the <strong>phase alignment</strong> between the wavelet and the signal.</p> <ul> <li>If the signal and wavelet are phase-aligned (peaks match peaks), the dot product is <strong>positive</strong>.</li> <li>If they are 90° out of phase (peaks match zero-crossings), the dot product is <strong>zero</strong>.</li> <li>If they are 180° out of phase (peaks match troughs), the dot product is <strong>negative</strong>.</li> </ul> <p><strong>Consequence:</strong> You cannot reliably estimate the “power” (energy) of the signal because the value fluctuates based on phase lag, not just signal strength.</p> <hr/> <h3 id="v-complex-wavelets--eulers-formula">V. Complex Wavelets &amp; Euler’s Formula</h3> <p>To extract stable estimates of Power and Phase independent of alignment, we use <strong>Complex Morlet Wavelets</strong>.</p> <h4 id="1-complex-sine-waves">1. Complex Sine Waves</h4> <p>Instead of a real sine wave, we use a complex sine wave defined by <strong>Euler’s Formula</strong>:</p> \[e^{i\theta} = \cos(\theta) + i\sin(\theta)\] <p>This creates a “corkscrew” shape that spirals through time in 3 dimensions (Time, Real, Imaginary).</p> <h4 id="2-the-complex-morlet-wavelet-formula">2. The Complex Morlet Wavelet Formula</h4> <p>Combining the Gaussian taper with the complex sine wave:</p> \[cmw(t) = A e^{-t^2 / (2s^2)} e^{i2\pi ft}\] <ul> <li><strong>Real Part:</strong> \( \cos(2\pi ft) \times \text{Gaussian} \)</li> <li><strong>Imaginary Part:</strong> \( \sin(2\pi ft) \times \text{Gaussian} \)</li> </ul> <hr/> <h3 id="vi-feature-extraction-geometry-of-the-dot-product">VI. Feature Extraction (Geometry of the Dot Product)</h3> <p>When you convolve a Complex Wavelet with neural data, the result at every time point is a <strong>Complex Number</strong>. This number describes a vector in the complex plane.</p> <p>From this single vector, we extract three distinct features:</p> <h4 id="1-bandpass-filtered-signal-real-axis">1. Bandpass Filtered Signal (Real Axis)</h4> <p>The projection of the vector onto the <strong>Real Axis</strong>. This is equivalent to applying a standard bandpass filter with a Gaussian frequency response.</p> <h4 id="2-instantaneous-power-vector-length">2. Instantaneous Power (Vector Length)</h4> <p>The <strong>Squared Magnitude</strong> of the vector. This tells you “how much” of that frequency is present, regardless of phase alignment.</p> <ul> <li><strong>Formula:</strong> \( \text{Power} = \text{abs}(Z)^2 \) or \( Z \cdot \text{conj}(Z) \)</li> <li><em>Note:</em> The conjugate method is computationally faster for large matrices.</li> </ul> <h4 id="3-instantaneous-phase-vector-angle">3. Instantaneous Phase (Vector Angle)</h4> <p>The <strong>Angle</strong> of the vector relative to the positive real axis. This tells you the timing of the oscillation.</p> <ul> <li><strong>Formula:</strong> \( \theta = \text{angle}(Z) = \arctan(\text{imag} / \text{real}) \)</li> </ul> <hr/> <h3 id="vii-the-uncertainty-principle-parameter--n-">VII. The Uncertainty Principle (Parameter ( n ))</h3> <p>The number of cycles \( n \) defines the width of the Gaussian taper. This parameter is governed by the Heisenberg uncertainty principle for time-frequency analysis: <strong>You cannot maximize both temporal and frequency precision simultaneously.</strong></p> <table> <thead> <tr> <th style="text-align: left">Cycles (( n ))</th> <th style="text-align: left">Gaussian Width</th> <th style="text-align: left">Temporal Precision</th> <th style="text-align: left">Frequency Precision</th> <th style="text-align: left">Best For…</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Low (e.g., 3)</strong></td> <td style="text-align: left">Narrow</td> <td style="text-align: left"><strong>High</strong> (Sharp timing)</td> <td style="text-align: left">Low (Broad spectrum)</td> <td style="text-align: left">Transient bursts / ERPs</td> </tr> <tr> <td style="text-align: left"><strong>High (e.g., 10)</strong></td> <td style="text-align: left">Wide</td> <td style="text-align: left">Low (Blurred timing)</td> <td style="text-align: left"><strong>High</strong> (Sharp spectrum)</td> <td style="text-align: left">Sustained oscillations</td> </tr> </tbody> </table> <h4 id="variable-cycles">Variable Cycles</h4> <p>It is common to use <strong>variable cycles</strong> (e.g., increasing from 3 to 10 as frequency increases). This adjusts the precision balance across the spectrum:</p> <ul> <li>Allows for better time precision at low frequencies.</li> <li>Allows for better frequency precision at high frequencies.</li> </ul> <p><strong>Source:</strong> Cohen, M. X. <em>Analyzing Neural Time Series Data: Theory and Practice</em>. The MIT Press, 2014. (Chapters 12 &amp; 13).</p>]]></content><author><name></name></author><category term="research,"/><category term="computational"/><category term="neuroscience"/><summary type="html"><![CDATA[Time-frequency decomposition using Analytic and Complex Morlet Wavelets]]></summary></entry><entry><title type="html">Fundamentals of Fourier Transform</title><link href="https://jong-min-moon.github.io/blog/2021/fourier/" rel="alternate" type="text/html" title="Fundamentals of Fourier Transform"/><published>2021-09-27T00:00:00+00:00</published><updated>2021-09-27T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2021/fourier</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2021/fourier/"><![CDATA[<h3 id="i-the-math-of-oscillations">I. The Math of Oscillations</h3> <p><strong>Context:</strong> Neural probes measure fluctuations in the excitability of neuronal populations. When plotted, these measurements form a time series curve containing repeated patterns known as <strong>oscillations</strong>. The Fourier transform decomposes these complex oscillations into a sum of basic sine waves.</p> <h4 id="1-components-of-an-oscillation">1. Components of an Oscillation</h4> <p>A sine oscillation is fully described by three pieces of information:</p> <ul> <li><strong>Frequency (Hz):</strong> How many times the pattern repeats in one second.</li> <li><strong>Power:</strong> The squared amplitude of the oscillation (strength of the signal).</li> <li><strong>Phase:</strong> The position along the sine wave cycle at a specific moment (e.g., peak, trough, rising slope).</li> </ul> <p><strong>Key Property:</strong> Power and phase are <strong>independent</strong> (Cohen, 2014, p. 31). Changing the strength of a signal does not alter its timing, and shifting its timing does not alter its strength.</p> <h4 id="2-mathematical-representation">2. Mathematical Representation</h4> <p>A sine wave can be represented mathematically as:</p> \[y(t) = A \sin(2\pi ft + \theta)\] <p>Where:</p> <ul> <li>\( A \): Amplitude (related to power).</li> <li>\( f \): Frequency.</li> <li>\( t \): Time.</li> <li>\( \theta \): Phase angle offset (value at \( t=0 \)).</li> </ul> <hr/> <h3 id="ii-the-fourier-transform">II. The Fourier Transform</h3> <p><strong>Core Concept:</strong> The Fourier transform is a <strong>basis expansion</strong> of the signal using sine waves as the “basis” functions. Instead of representing the signal as a sequence of data points over time, we represent it as a sequence of sine basis coefficients.</p> <h4 id="1-the-3d-result">1. The 3D Result</h4> <p>Because a sine wave is fully described by frequency, power, and phase, the result of a Fourier transform is a <strong>3-dimensional representation</strong> of the time series:</p> <ol> <li>Frequency (Dimension 1)</li> <li>Power (Dimension 2)</li> <li>Phase (Dimension 3)</li> </ol> <p><strong>Note:</strong> This 3D representation contains <strong>ALL</strong> information from the original time series. You can losslessly convert between the time domain and this frequency domain.</p> <h4 id="2-visualization-the-power-spectrum">2. Visualization: The Power Spectrum</h4> <p>While the result is 3D, we often ignore the phase information for simple visualization. The most typical plot is a <strong>2-D Power Spectrum</strong>:</p> <ul> <li><strong>X-axis:</strong> Frequency</li> <li><strong>Y-axis:</strong> Power (or Amplitude)</li> </ul> <p>This plot essentially shows the magnitude of the coefficients for each sine wave basis function.</p> <h4 id="3-the-inverse-fourier-transform">3. The Inverse Fourier Transform</h4> <p>This is simply the reverse process: expanding the signal using the basis functions to recover the original time series. It sums the weighted sine waves back together to recreate the raw data.</p> <h4 id="4-critical-assumption-stationarity">4. Critical Assumption: Stationarity</h4> <p>The Fourier transform assumes that the data are <strong>stationary</strong>.</p> <ul> <li><strong>Definition:</strong> The statistics of the signal—mean, variance, and frequency structure—do not change over time.</li> <li><strong>Implication:</strong> The time series must be “well-behaved” for the standard Fourier transform to be valid. If a brain signal changes frequency rapidly (non-stationary), other methods (like Wavelets or Hilbert transform) may be required.</li> </ul> <h3 id="iii-the-convolution-theorem">III. The Convolution Theorem</h3> <p><strong>Core Concept:</strong> Convolution in the time domain is mathematically equivalent to multiplication in the frequency domain.</p> \[\text{Convolution}(Signal, Kernel) \longleftrightarrow \text{FFT}(Signal) \times \text{FFT}(Kernel)\] <h4 id="1-mechanism-spectral-scaling">1. Mechanism: Spectral Scaling</h4> <p>When you perform the frequency-by-frequency multiplication of the Fourier transforms of a kernel and a signal, you are effectively <strong>scaling</strong> the frequency spectrum of the signal by the frequency spectrum of the kernel.</p> <ul> <li><strong>If the kernel has high power at 10 Hz:</strong> The 10 Hz component of the signal is retained (multiplied by a large number).</li> <li><strong>If the kernel has zero power at 50 Hz:</strong> The 50 Hz component of the signal is eliminated (multiplied by zero).</li> </ul> <h4 id="2-interpretation-the-common-structure">2. Interpretation: The Common Structure</h4> <p>The result of this multiplication (and hence, the result of the convolution) represents the <strong>frequency structure that is common to both the kernel and the signal</strong>.</p> <h4 id="3-conclusion-convolution-as-a-filter">3. Conclusion: Convolution as a Filter</h4> <p>Thus, convolution acts as a <strong>frequency filter</strong>. The frequency profile of the signal is “passed through” the frequency profile of the kernel, allowing only the overlapping frequencies to survive. In a more intuitive way, filtering is just doing Fourier transform and killing the coefficients of frequencies you don’t want. This is elgantly explained as convolution with kernel in time domain.</p> <p><strong>Source:</strong> Cohen, M. X. <em>Analyzing Neural Time Series Data: Theory and Practice</em>. The MIT Press, Cambridge, Massachusetts, 2014. (Chapter 11).</p>]]></content><author><name></name></author><category term="research,"/><category term="computational"/><category term="neuroscience"/><summary type="html"><![CDATA[From neural oscillations to frequency domain analysis]]></summary></entry><entry><title type="html">Fundamentals of Convolution</title><link href="https://jong-min-moon.github.io/blog/2021/convolution/" rel="alternate" type="text/html" title="Fundamentals of Convolution"/><published>2021-09-13T00:00:00+00:00</published><updated>2021-09-13T00:00:00+00:00</updated><id>https://jong-min-moon.github.io/blog/2021/convolution</id><content type="html" xml:base="https://jong-min-moon.github.io/blog/2021/convolution/"><![CDATA[<h3 id="i-signal-processing-perspective">I. Signal Processing Perspective</h3> <p><strong>Core Concept:</strong> Convolution is an extension of the inner product, representing the time-varying similarity between a signal and a kernel (filter).</p> <h4 id="1-mathematical-foundation-the-inner-product">1. Mathematical Foundation: The Inner Product</h4> <ul> <li><strong>Definition:</strong> The scalar result obtained by summing the products of corresponding elements in two vectors.</li> <li><strong>Interpretation:</strong> The inner product represents the <strong>similarity</strong> between two vectors. <ul> <li>High inner product: High similarity.</li> <li>Zero inner product: Orthogonality (no similarity).</li> </ul> </li> </ul> <h4 id="2-the-convolution-operation">2. The Convolution Operation</h4> <ul> <li><strong>Definition:</strong> A repeated computation of the inner product over time.</li> <li><strong>Procedure:</strong> <ol> <li><strong>Flip:</strong> The kernel (the mover or filter) is reversed in time.</li> <li><strong>Shift:</strong> The flipped kernel slides along the time axis of the signal.</li> <li><strong>Multiply &amp; Sum:</strong> At each time step, the inner product is computed between the signal and the overlapping kernel section.</li> </ol> </li> <li><strong>Result:</strong> A time series representing the similarity between the signal and the flipped kernel at every time point.</li> </ul> <h4 id="3-convolution-vs-cross-covariance">3. Convolution vs. Cross-Covariance</h4> <ul> <li><strong>The “Flip” Distinction:</strong> <ul> <li><strong>Convolution:</strong> Requires the kernel to be flipped (reversed) relative to the time axis.</li> <li><strong>Cross-Covariance:</strong> The kernel is <em>not</em> flipped; it is simply shifted and the inner product is computed.</li> </ul> </li> <li><strong>Note:</strong> If the kernel is symmetric (e.g., a Gaussian distribution or cosine wave), convolution and cross-covariance yield mathematically identical results, though they remain conceptually distinct operations.</li> </ul> <h4 id="4-interpretations-of-convolution">4. Interpretations of Convolution</h4> <p>Cohen (2014) offers standard interpretations:</p> <ul> <li><strong>Signal Processing:</strong> One signal acting as a weight for another signal that slides along it.</li> <li><strong>Statistical:</strong> A cross-covariance (assuming symmetry or accounting for the flip).</li> <li><strong>Geometric:</strong> A time series of mappings between two vectors.</li> <li><strong>Functional:</strong> A <strong>frequency filter</strong> (isolating specific frequencies in the time domain).</li> </ul> <hr/> <h3 id="ii-statistical-perspective">II. Statistical Perspective</h3> <p><strong>Core Concept:</strong> Convolution defines the probability distribution of the <strong>sum</strong> of independent random variables.</p> <h4 id="1-sum-of-random-variables">1. Sum of Random Variables</h4> <ul> <li>Given two independent random variables \( X \) and \( Y \) with probability density functions (PDFs) \( f_X \) and \( f_Y \).</li> <li>Let \( Z = X + Y \).</li> <li>The PDF of \( Z \) is the convolution of the PDFs of \( X \) and \( Y \):</li> </ul> \[f_Z(z) = (f_X \ast f_Y)(z)\] <h4 id="2-the-integral-formulation">2. The Integral Formulation</h4> \[f_Z(z) = \int_{-\infty}^{\infty} f_X(x) \cdot f_Y(z-x) \, dx\] <ul> <li><strong>Connection to Signal Processing:</strong> <ul> <li>The term \( f_Y(z-x) \) contains the same “Flip and Shift” mechanics found in signal theory.</li> <li><strong>Flip:</strong> \( -x \) (the variable is negated/reversed).</li> <li><strong>Shift:</strong> \( z \) (the variable is shifted by the total sum).</li> </ul> </li> </ul> <hr/> <h3 id="iii-common-properties-smoothing">III. Common Properties: Smoothing</h3> <p>In both domains, convolution acts as a smoothing operator.</p> <ul> <li><strong>In Signals:</strong> Convolving a sharp signal with a broad kernel smoothes out high-frequency noise (Low-pass filtering).</li> <li><strong>In Statistics:</strong> Adding random variables increases uncertainty (variance). The resulting distribution \( Z \) is wider and flatter than the constituent distributions \( X \) or \( Y \).</li> </ul> <p><strong>Source:</strong> Cohen, M. X. <em>Analyzing Neural Time Series Data: Theory and Practice</em>. The MIT Press, Cambridge, Massachusetts, 2014. (Chapter 10).</p>]]></content><author><name></name></author><category term="research,"/><category term="computational"/><category term="neuroscience"/><summary type="html"><![CDATA[Inner product and convolution in signal processing and statistics]]></summary></entry></feed>
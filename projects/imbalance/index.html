<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Wildfire Prediction from Imbalanced Data | Jongmin Mun</title> <meta name="author" content="Jongmin Mun"> <meta name="description" content="Predicting forest fire from artillery training data and weather data"> <meta name="keywords" content="Jongmin Mun, USC"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?v=1771975563"> <link rel="canonical" href="https://jong-min-moon.github.io/projects/imbalance/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jongmin </span>Mun</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Study</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/coursework/">Coursework</a> <a class="dropdown-item" href="/study/papers/">Papers</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Posts</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Wildfire Prediction from Imbalanced Data</h1> <p class="post-description">Predicting forest fire from artillery training data and weather data</p> </header> <article> <h3 id="1-data-collection-and-preprocessing">1. Data Collection and Preprocessing</h3> <p>The study combines two distinct datasets to create a predictive model:</p> <ul> <li> <strong>ROKA Dataset:</strong> Obtained from the Republic of Korea Army, this dataset records 984 artillery training sessions over five years. It provides the binary response variable: wildfire occurrence ($1$) or non-occurrence ($-1$). The data is highly imbalanced, with a ratio of approximately 1:24 (40 fires vs. 944 non-fires).</li> <li> <strong>SKMA Dataset:</strong> Sourced from the South Korea Meteorological Administration, this provides meteorological predictor variables including temperature, precipitation, wind speed, and relative humidity.</li> <li> <strong>Data Merging and Normalization:</strong> The datasets were linked via temporal information. To account for the distance between weather stations and training sites, the study used Barnes interpolation to create a gridified weather map. Additionally, a Digital Elevation Model (DEM) was used to normalize temperature data based on the altitude differences between observation stations and training sites. All predictor variables were normalized to have a zero mean and a standard deviation of one.</li> </ul> <h3 id="2-proposed-algorithm-gc-wsvm">2. Proposed Algorithm: GC-WSVM</h3> <p>The authors proposed a method called <strong>Gaussian mixture clustering weighted SVM (GC-WSVM)</strong>, which operates in two steps to handle the skewed class distribution:</p> <ul> <li> <strong>Step 1: GMM-Based Oversampling:</strong> <ul> <li>The study utilized a Gaussian Mixture Model (GMM) to estimate the probability distribution of the minority class (wildfire occurrences).</li> <li>Synthetic samples were generated from this learned distribution to balance the dataset.</li> <li>GMM was chosen over the traditional SMOTE algorithm because the minority class samples exhibited a clustered structure, and SMOTE’s linear interpolation could incorrectly generate samples in the empty space between subgroups.</li> </ul> </li> <li> <strong>Step 2: Weighted Support Vector Machine (WSVM):</strong> <ul> <li>A weighted Gaussian kernel SVM was applied to the balanced dataset.</li> <li>The algorithm assigned different misclassification costs (weights) to the classes.</li> <li>To prioritize the detection of the minority class, the weights were set proportional to the inverse of the number of samples; specifically, the cost for misclassifying a wildfire ($1/n_{min}^*$) was set higher than that for non-occurrence ($1/n_{maj}$).</li> </ul> </li> </ul> <h3 id="3-experimental-design-and-validation">3. Experimental Design and Validation</h3> <ul> <li> <strong>Comparison:</strong> The performance of GC-WSVM was compared against Linear SVM and standard Gaussian Kernel SVM.</li> <li> <strong>Validation Method:</strong> Due to the scarcity of minority samples, the study employed fivefold cross-validation rather than a simple train-test split.</li> <li> <strong>Hyperparameter Tuning:</strong> A grid search was conducted to tune hyperparameters, including the cost parameter $C$, the Gaussian kernel bandwidth $\gamma$, and the desired imbalance ratio $R$.</li> <li> <strong>Metrics:</strong> The model performance was evaluated using Sensitivity, Specificity, and the $G-mean$ (the geometric mean of sensitivity and specificity), as the latter provides a balanced view of performance on both majority and minority classes. rather than a simple train-test split[cite: 424, 425].</li> <li>[cite_start]<strong>Hyperparameter Tuning:</strong> A grid search was conducted to tune hyperparameters, including the cost parameter $C$, the Gaussian kernel bandwidth $\gamma$, and the desired imbalance ratio $R$[cite: 430, 431].</li> <li>[cite_start]<strong>Metrics:</strong> The model performance was evaluated using Sensitivity, Specificity, and the $G-mean$ (the geometric mean of sensitivity and specificity), as the latter provides a balanced view of performance on both majority and minority classes[cite: 185, 186].</li> </ul> <h1 id="challenges-and-methods">Challenges and Methods</h1> <h2 id="1-xy-axis-interpolation-barnes-interpolation">1. XY-axis Interpolation: Barnes Interpolation</h2> <p><strong>Challenge:</strong> A significant spatial discrepancy exists between KMA weather observatories and artillery training sites. To accurately estimate meteorological conditions at specific training locations, spatial interpolation is required.</p> <p><strong>Method Intuition:</strong> The study employs <strong>Barnes Interpolation</strong>, which functions essentially as a <strong>two-step Nadaraya-Watson (NW) estimation</strong> process. It addresses the high bias (over-smoothing) typical of a single kernel smoother by introducing a second estimator to correct local errors.</p> <h3 id="step-1-the-initial-estimate-base-learner">Step 1: The Initial Estimate (Base Learner)</h3> <p>The first pass utilizes a standard <strong>Nadaraya-Watson estimator</strong> with a Gaussian kernel.</p> <ul> <li> <strong>Goal:</strong> Capture the field’s <strong>global trends</strong> (e.g., general pressure systems).</li> <li> <strong>Mechanism:</strong> Estimates the value at any grid point, $g_0(x)$, via a weighted average of surrounding observed data points, $f_k$.</li> <li> <strong>The Kernel:</strong> Weights are determined by a <strong>Gaussian kernel</strong> with bandwidth $\kappa$. <ul> <li>$g_0(x) \approx \text{Weighted Average of Neighbors}$</li> </ul> </li> </ul> <h3 id="step-2-the-correction-residual-learner">Step 2: The Correction (Residual Learner)</h3> <p>The second pass applies a <strong>Nadaraya-Watson estimator to the residuals</strong>.</p> <ul> <li> <strong>Goal:</strong> Recover <strong>local, high-frequency details</strong> (e.g., small storm pockets) smoothed over by the first pass.</li> <li> <strong>Mechanism:</strong> <ol> <li>Calculate the <strong>residual</strong> at every original data point: \(Error = \text{Observed Value} - \text{First Pass Estimate}\)</li> <li>Apply a second NW estimator to these <em>error values</em>.</li> </ol> </li> <li> <strong>The Kernel Adjustment:</strong> This pass uses a <strong>narrower bandwidth</strong> (scaled by $\gamma$, where $0.2 &lt; \gamma &lt; 1.0$) to capture the sharp, local variations missed by the broader first kernel.</li> </ul> <h3 id="final-result">Final Result</h3> <p>The final interpolated map combines the global trend with the local correction:</p> \[\text{Final Map} = \underbrace{\text{First NW Estimate}}_{\text{Global Trend}} + \underbrace{\text{Second NW Estimate}}_{\text{Local Correction}}\] <h2 id="2-z-axis-interpolation-for-temperature">2. Z-axis Interpolation for Temperature</h2> <p><strong>Challenge:</strong> In addition to horizontal distance, the <strong>altitude</strong> of weather stations often differs significantly from that of the training sites. Since temperature changes with elevation, using raw station data without adjustment would introduce significant error.</p> <p><strong>Method Intuition:</strong></p> <ul> <li> <p><strong>Step 1: Acquiring Elevation Data</strong> Since the Army dataset lacks altitude information, the study utilizes an open-source digital topographic database of Earth from the Shuttle Radar Topography Mission (SRTM). This dataset provides high-resolution elevation data (30 meters) <em>(Source: <a href="https://portal.opentopography.org/raster?opentopoID=OTSRTM.082015.4326.1" rel="external nofollow noopener" target="_blank">OpenTopography</a>)</em></p> </li> <li> <p><strong>Step 2: Lapse Rate Adjustment</strong> The temperature recorded at the observation station is adjusted using a standard <strong>lapse rate of $0.65^\circ\text{C} / 100\text{m}$</strong>.</p> </li> </ul> <h2 id="3-minority-class-oversampling">3. Minority class oversampling</h2> <h3 id="smote-synthetic-minority-oversampling-technique">SMOTE (Synthetic Minority Oversampling Technique)</h3> <p>For each actual minority class sample (let’s call it <strong>Point A</strong>), given a tuning paramete $k$:</p> <ol> <li> <strong>Find Neighbors:</strong> It looks around <strong>Point A</strong> and identifies its $k$ nearest minority class neighbors.</li> <li> <strong>Pick a Partner:</strong> It randomly chooses <strong>one</strong> of those neighbors (let’s call it <strong>Point B</strong>).</li> <li> <strong>Draw a Line:</strong> Imagine drawing a straight line connecting <strong>Point A</strong> and <strong>Point B</strong>.</li> <li> <strong>Create a New Point:</strong> The algorithm picks a random spot somewhere along that line and places a new, synthetic point there.</li> </ol> <h4 id="why-the-authors-rejected-it">Why the Authors Rejected It</h4> <p>Visual inspection revealed that the wildfire data was clustered into subgroups. This creates a risk for SMOTE: if <strong>Point A</strong> belongs to one subgroup and one of its $k$ “nearest neighbors” (<strong>Point B</strong>) belongs to a separate subgroup nearby, SMOTE will draw a line connecting them through the empty space between clusters. Any synthetic point generated along this line would likely be unrealistic, representing a data point that does not statistically resemble a real wildfire event.</p> <h3 id="gaussian-mixture-model-gmm">Gaussian Mixture Model (GMM)</h3> <p>GMM is a probabilistic model that assumes all data points are generated from a mixture of a finite number of Gaussian (normal) distributions with unknown parameters.</p> <ul> <li> <strong>In this paper:</strong> The authors observed that the minority class (wildfire occurrences) formed several distinct clusters rather than a single group.</li> <li> <strong>Application:</strong> Instead of assuming a single distribution, they used GMM to estimate the probability distribution of these minority samples. They then generated synthetic data from this learned distribution to oversample the minority class and balance the dataset.</li> </ul> <h3 id="5-svm-support-vector-machine">5. SVM (Support Vector Machine)</h3> <p>Support Vector Machine (SVM) is a supervised classifier that identifies the optimal hyperplane ($w \cdot x + b$) by optimizing the sum of two conflicting objectives: hinge loss and squared $\ell_2$ regularization. The hinge loss penalizes the model for misclassifying points; it earns its name because correctly classifying points (outside the margin) incurs zero loss, whereas misclassification incurs a linear loss proportional to the distance from the boundary. Meanwhile, the squared regularization minimizes the norm of the weights ($|w|^2$), which is mathematically equivalent to maximizing the margin—the geometric distance between the decision boundary and the nearest data points. Notably, since the mathematical formulation relies entirely on inner products between data points, the linear decision boundary can be extended to capture nonlinear relationships by replacing these inner products with a kernel—a bivariate, nonlinear function that maps two points to a measure of their similarity in a higher-dimensional space.</p> <h3 id="7-weighted-svm">7. Weighted SVM</h3> <p>To prevent the model from overlooking rare wildfire incidents, the weighted SVM modifies the learning algorithm by adjusting the hyperplane towards the minority class, which helps classify more samples as part of that group. It does this by adjusting the slope of the hinge loss function separately for the positive and negative classes. Specifically, the weights are set proportional to the inverse of the number of original samples, effectively making the model “pay more” for misclassifying a wildfire.</p> <p>Furthermore, our follow-up research proposes utilizing three distinct hinge loss functions corresponding to the original majority class, the original minority class, and the synthetic minority class. The slopes of these losses are scaled according to the ratio of the total majority class to the total minority class, and the ratio of synthetic minority samples to original minority samples.</p> <p>We demonstrate that as the original sample size approaches infinity—assuming the Gaussian Mixture Model sufficiently approximates the minority class distribution and the oversampling and weighting policies remain fixed—this SVM converges to the Bayes optimal classifier. This result establishes statistical <strong>consistency</strong> rather than optimality; it satisfies the minimum requirement of a learning algorithm: given infinite data, the algorithm recovers the best theoretically possible decision boundary (the Bayes classifier). The proof follows a two-step logic:</p> <ol> <li> <strong>Reduction to 1D Optimization:</strong> Utilizing the properties of Reproducing Kernel Hilbert Spaces (RKHS), the optimization over the population expectation is reduced to a one-dimensional optimization problem on the real line. This reduction generalizes the findings of Lin et al. (2002) to our specific framework incorporating three hinge losses.</li> <li> <strong>ERM Convergence:</strong> Treating the SVM as an Empirical Risk Minimization (ERM) problem, we demonstrate that the solution to the ERM converges to the solution of the population optimization as approaches infinity.</li> </ol> <h1 id="details-of-the-army-dataset">Details of the Army dataset</h1> <p>The dataset tracks several key variables for each incident:</p> <ul> <li> <strong>Date:</strong> This captures the specific date and time of the event. The provided examples—spanning October, November, April, and March—align with the study’s finding that the vast majority of military wildfires (39 out of 40) occur during the dry seasons of February–April and October–November[cite: 196, 197, 237].</li> <li> <strong>Location:</strong> The administrative district where the training occurred is recorded, such as <em>Goseong-gun</em> or <em>Paju-si</em>.</li> <li> <strong>Shooting Type:</strong> This details the specific munition or weapon system that triggered the fire. Examples include: <ul> <li> <em>Red parachute flare</em> and <em>Star shell</em> (illumination rounds).</li> <li> <em>60 mm trench mortar</em> (artillery).</li> <li> <em>Panzerfaust3</em> (anti-tank warhead).</li> </ul> </li> <li> <strong>Damaged Area:</strong> This measures the extent of the forest affected by the fire, with values in the provided examples ranging from 2,500 to 150,000.</li> </ul> <h3 id="variable-exclusion-and-future-work">Variable Exclusion and Future Work</h3> <p>It is important to note that while <strong>Location</strong>, <strong>Shooting Type</strong>, and <strong>Damaged Area</strong> appear in the raw data, the authors <strong>excluded</strong> them from the final predictive model. <em>Location</em> and <em>Shooting Type</em> were removed to focus the model strictly on meteorological conditions, while <em>Damaged Area</em> was ignored because the model functions as a binary classifier (predicting <em>occurrence</em> vs. <em>non-occurrence</em>) rather than a regression model predicting severity. However, the authors suggest that incorporating these variables could enhance predictive power and remains a potential avenue for future research.</p> <h3 id="sample-data">Sample data</h3> <p>| No. | Date | Location | Cause (Shooting Type) | Damage Size | Fire Occurred | | :— | :— | :— | :— | :— | :— | | <strong>1</strong> | Oct 31, 2019 | Goseong-gun | Red parachute flare | 2,500 | 1 | | <strong>2</strong> | Nov 8, 2019 | Goseong-gun | Star shell | 3,000 | 1 | | <strong>3</strong> | Apr 13, 2017 | Paju-si | 60 mm trench mortar | 150,000 | 1 | | <strong>4</strong> | Mar 22, 2018 | Paju-si | Panzerfaust3 | 3,300 | 1 |</p> </article><h2>References</h2> <div class="publications"> <h2 class="bibliography">class imbalance</h2> <ol class="bibliography"><li> <div class="row grid-item" data-category='"class imbalance"'> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/forest_fire-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/forest_fire-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/forest_fire-1400.webp"></source> <img src="/assets/img/publication_preview/forest_fire.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="forest_fire.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nam_prediction_2024" class="col-sm-8"> <div class="title">Prediction of forest fire risk for artillery military training using weighted support vector machine for imbalanced data</div> <div class="author"> Ji Hyun Nam, Jongmin Mun, Seongil Jo, and Jaeoh Kim</div> <div class="periodical"> <em>Journal of Classification</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Summary</a> <a href="https://doi.org/10.1007/s00357-024-09467-1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Published Version</a> </div> <div class="abstract hidden"> <p> Artillery training inherently poses wildfire risks. Predictive modeling of these wildfires faces two key challenges: the scarcity of wildfire cases and the limited granularity of meteorological data during training. We address the first challenge by augmenting the data using a Gaussian mixture generative model and adjusting the loss function of a support vector machine. To tackle the second challenge, we integrate the Republic of Korea Army (ROKA) dataset with the Korea Meteorological Administration database. Our resulting model achieves a 99% improvement in balanced classification metrics compared to previous models. </p> </div> </div> </div> </li></ol> <h2 class="bibliography">class imbalance</h2> <ol class="bibliography"><li> <div class="row grid-item" data-category='"class imbalance"'> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/wsvm-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/wsvm-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/wsvm-1400.webp"></source> <img src="/assets/img/publication_preview/wsvm.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="wsvm.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mun_weighted_2025" class="col-sm-8"> <div class="title">Weighted support vector machine for extremely imbalanced data</div> <div class="author"> Jongmin Mun, Sungwan Bang, and Jaeoh Kim</div> <div class="periodical"> <em>Computational Statistics &amp; Data Analysis</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Summary</a> <a href="https://doi.org/10.1016/j.csda.2024.108078" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Published Version</a> </div> <div class="abstract hidden"> <p> Data augmentation and loss function adjustments are two common techniques for imbalanced classification. In cases of extreme class imbalance, it is often standard practice to combine these two approaches. However, determining the optimal oversampling ratio and the degree of asymmetry in the loss function typically relies on heuristics. We further consider the scenario where the minority class consists of subgroups and propose a straightforward method for combining data augmentation and loss function adjustments. This method serves as a sample-based approximation of the population-level asymptotically Bayes optimal oracle procedure. </p> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Jongmin Mun. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
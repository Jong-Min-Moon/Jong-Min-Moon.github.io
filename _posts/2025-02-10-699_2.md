---
layout: distill
title: a distill-style blog post
description: an example of a distill-style blog post and main elements
tags: distill formatting
giscus_comments: true
date: 2021-05-22
featured: true

authors:
  - name: Albert Einstein
    url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    affiliations:
      name: IAS, Princeton
  - name: Boris Podolsky
    url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
    affiliations:
      name: IAS, Princeton
  - name: Nathan Rosen
    url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
    affiliations:
      name: IAS, Princeton

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Equations
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Interactive Plots
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---
## Do not invert a matrix to run OLS
The reason is, computing and inverting \(\mathbf{X}^\top \mathbf{X}\) is both costly and error-prone.

The ordinary least-squares estimate is found by minimizing $S(\boldsymbol{\beta}) = (y - \mathbf{X}\boldsymbol{\beta})^T (y - \mathbf{X} \boldsymbol{\beta})$. Since this is an unconstrained quadratic optimization problem, the necessary and sufficient condition for a minimizer is that the gradient is zero. Therefore, we get the solution by solving the following equation:

\[
\mathbf{X}^\top \mathbf{X} \boldsymbol{\beta} = \mathbf{X}^\top \mathbf{y}.
\]
However, there are reasons not to compute \(\hat{\boldsymbol{\beta}}\) this way:
- Forming the matrix \(X^T X\) typically introduces errors due to the \(p^2 n\) floating point multiplications and additions required.
- Computing a matrix inverse and multiplying by it introduces additional floating point errors and is computationally expensive.

Instead, we apply the thin QR decomposition. Assuming \( n > p \), no multicollinearity, and that we are not performing ANOVA, the design matrix \( \mathbf{X} \) is full rank. we decompose  the $n \times p$ matrix $\mathbf{X}$ into an  $n \times p$ orthonormal matrix $\mathbf{Q}$ and a $p \times p$ upper triangular full-rank matrix $\mathbf{R}$.



## Compute the fitted values using thin QR decomposition

\[
\begin{align*}
\hat{\mathbf{y}} &= \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
\\&= \mathbf{Q} \mathbf{R} (\mathbf{R}^\top \mathbf{Q}^\top \mathbf{Q} \mathbf{R})^{-1} \mathbf{R}^\top \mathbf{Q}^\top \mathbf{y}
%%%%%%%%%%%%%%%%%%
\\&=\mathbf{Q} \mathbf{R} (\mathbf{R}^\top \mathbf{R})^{-1} \mathbf{R}^\top \mathbf{Q}^\top \mathbf{y}
%%%%%%%%%%%%%%
\\&= \mathbf{Q} \mathbf{Q}^\top \mathbf{y}
\end{align*}
\]

## Estimate the regression coefficient using thin QR decomposition
Since \( \mathbf{X} = \mathbf{QR} \) and \( \mathbf{X}^\top = \mathbf{R}^\top \mathbf{Q}^\top\), we have \( \mathbf{X}^\top \mathbf{X} = \mathbf{R}^\top \mathbf{Q}^\top \mathbf{Q} \mathbf{R} = \mathbf{R}^\top \mathbf{R} \). Therefore we have:
\[
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top  \mathbf{y}= (\mathbf{R}^\top \mathbf{R})^{-1} \mathbf{R}^\top \mathbf{Q}^\top\mathbf{y}.
  \]
Thus $\hat{\boldsymbol{\beta}}$ is the solution of the following linear equation:
\[
  \mathbf{R}^\top \mathbf{R}\boldsymbol{\beta} = \mathbf{R}^\top \mathbf{Q}^\top\mathbf{y}.
  \]
Since $\mathbf{R}$ is full-rank, it is invertible. Thus we can cancel it and instead solve:
\[
  \mathbf{R}\boldsymbol{\beta} =  \mathbf{Q}^\top\mathbf{y}.
  \]
Thus, one multiplication followed by solving a single triangular system is enough. This is one standard way that linear regression is typically done in statistical computing packages (Splus, R, etc.).

<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Augmented IPW and Double Robustness | Jongmin Mun</title> <meta name="author" content="Jongmin Mun"> <meta name="description" content="AIPW estimator, double robustness, and cross-fitting"> <meta name="keywords" content="Jongmin Mun, USC"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?v=1772159068"> <link rel="canonical" href="https://jong-min-moon.github.io/blog/2025/augmented-ipw-and-double-robustness/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Augmented IPW and Double Robustness",
      "description": "AIPW estimator, double robustness, and cross-fitting",
      "published": "October 15, 2025",
      "authors": [
        {
          "author": "Jong Min Moon",
          "authorURL": "https://github.com/Jong-Min-Moon",
          "affiliations": [
            {
              "name": "USC Marshall",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jongmin </span>Mun</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Study</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/coursework/">Coursework</a> <a class="dropdown-item" href="/study/papers/">Papers</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Posts<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill "> <d-title> <h1>Augmented IPW and Double Robustness</h1> <p>AIPW estimator, double robustness, and cross-fitting</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#statistical-setting">Statistical setting</a></div> <div><a href="#two-characterizations-of-the-ate">Two characterizations of the ATE</a></div> <div><a href="#augmented-ipw">Augmented IPW</a></div> <div><a href="#weak-double-robustness">Weak double robustness</a></div> <div><a href="#strong-double-robustness">Strong double robustness</a></div> <div><a href="#cross-fitting">Cross-fitting</a></div> <div><a href="#condensed-notation">Condensed notation</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>We saw that IPW is a simple ATE stimator under unconfoundedness, which is insensitive to the regression model misspecification. However, the large-sample properties of IPW are not particularly good enough.</p> <h2 id="two-characterizations-of-the-ate">Two characterizations of the ATE</h2> <p>The ATE can be characterized in terms of the propensity score \(e(x) = \mathbb{P} [W_i = 1 \mid X_i = x]\), as the expectation of oracle IPW estimator:</p> \[\begin{equation*} \tau = \mathbb{E} [\hat{\tau}^*_{IPW}] , \quad \hat{\tau}^*_{IPW} = \frac{1}{n} \sum_{i=1}^n \left( \frac{W_i Y_i}{e(X_i)} - \frac{(1 - W_i) Y_i}{1 - e(X_i)} \right). \end{equation*}\] <p>However, $\tau$ can also be characterized in terms of the conditional response surfaces $\mu_{(w)}(x) = \mathbb{E} [Y_i(w) \mid X_i = x]$. Under unconfoundedness,</p> \[\begin{align*} \tau(x) &amp;:= \mathbb{E} [Y_i(1) - Y_i(0) \mid X_i = x] \\ &amp;= \mathbb{E} [Y_i(1) \mid X_i = x] - \mathbb{E} [Y_i(0) \mid X_i = x] \\ &amp;= \mathbb{E} [Y_i(1) \mid X_i = x, W_i = 1] - \mathbb{E} [Y_i(0) \mid X_i = x, W_i = 0] \quad &amp;(\text{unconf}) \\ &amp;= \mathbb{E} [Y_i \mid X_i = x, W_i = 1] - \mathbb{E} [Y_i \mid X_i = x, W_i = 0] \quad &amp;(\text{SUTVA}) \\ &amp;= \mu_{(1)}(x) - \mu_{(0)}(x), \end{align*}\] <p>and so $\tau = \mathbb{E} [\mu_{(1)}(x) - \mu_{(0)}(x)]$. Thus we could also derive a consistent (but not necessarily optimal) estimator for $\tau$ by first estimating $\mu_{(0)}(x)$ and $\mu_{(1)}(x)$ non-parametrically, and then using \(\begin{equation*} \hat{\tau}_{REG} = \frac{1}{n} \sum_{i=1}^n(\hat{\mu}_{(1)}(X_i) - \hat{\mu}_{(0)}(X_i)). \end{equation*}\)</p> <h2 id="augmented-ipw">Augmented IPW</h2> <p>AIPW mixes the two characterization of ATE by first making a best effort attempt at \(\tau\) by estimating \(\mu_{(0)}(x)\) and \(\mu_{(1)}(x)\); then, it deals with any biases of the \(\hat{\mu}_{(w)}(x)\) by applying IPW to the regression residuals<d-cite key="robinsEstimationRegressionCoefficients1994"></d-cite>:</p> <p> $$ \begin{align*} \hat{\tau}_{AIPW} &amp;= \frac{1}{n} \sum_{i=1}^n \left( \hat{\mu}_{(1)}(X_i) - \hat{\mu}_{(0)}(X_i) + \frac{W_i (Y_i - \hat{\mu}_{(1)}(X_i))}{\hat{e}(X_i)} - \frac{(1 - W_i)(Y_i - \hat{\mu}_{(0)}(X_i))}{1 - \hat{e}(X_i)} \right). \end{align*} $$ </p> <h2 id="weak-double-robustness-convergence-in-probability-to-ate">Weak double robustness: convergence in probability to ATE</h2> <p>This means that AIPW is consistent if either the $\hat{\mu}<em>{(w)}(x)$ are consistent or $\hat{e}(x)$ is consistent. To see this, first consider the case where $\hat{\mu}</em>{(w)}(x)$ is consistent, the regression term goes to $\tau$ and the propensity term also goes to zero because of the residualization:</p> <p> $$ \begin{align*} \hat{\tau}_{AIPW} &amp;= \frac{1}{n} \sum_{i=1}^n (\hat{\mu}_{(1)}(X_i) - \hat{\mu}_{(0)}(X_i)) \quad &amp;\text{ (a consistent treatment effect estimator)} \\ &amp;+ \frac{1}{n} \sum_{i=1}^n \left( \frac{W_i}{\hat{e}(X_i)}(Y_i - \hat{\mu}_{(1)}(X_i)) - \frac{1 - W_i}{1 - \hat{e}(X_i)}(Y_i - \hat{\mu}_{(0)}(X_i)) \right) \quad &amp;\text{ ($\approx$ mean-zero noise)}, \end{align*} $$ </p> <p>Second, suppose that $\hat{e}(x)$ is consistent, i.e., $\hat{e}(x) \approx e(x)$. We can express the AIPW estiamtor alternatively as IPW estiamtor plus regression estimators weighted by propensity residuals. Then, the IPW term goes to $\tau$ and the regression term also goes to zero because of the residualization:</p> <p> $$ \begin{align*} \hat{\tau}_{AIPW} &amp;= \frac{1}{n} \sum_{i=1}^n \left( \frac{W_iY_i}{\hat{e}(X_i)} - \frac{(1 - W_i)Y_i}{1 - \hat{e}(X_i)} \right) \quad &amp;\text{ (the IPW estimator)} \\ &amp;+ \frac{1}{n} \sum_{i=1}^n \left( \hat{\mu}_{(1)}(X_i) \left( 1 - \frac{W_i}{\hat{e}(X_i)} \right) - \hat{\mu}_{(0)}(X_i) \left( 1 - \frac{1 - W_i}{1 - \hat{e}(X_i)} \right)\right) \quad &amp;\text{ ($\approx$ mean-zero noise)}, \end{align*} $$ </p> <h2 id="strong-double-robustness-convergence-speed-clt">Strong double robustness: convergence speed (CLT)</h2> <p>Consider the following “oracle” AIPW estimator that depends on the true $\mu_{(w)}(x)$ and $e(x)$ rather than on estimates thereof:</p> <p> $$ \begin{equation*} \hat{\tau}^*_{AIPW} = \frac{1}{n} \sum_{i=1}^n \left( \mu_{(1)}(X_i) - \mu_{(0)}(X_i) + \frac{W_i(Y_i - \mu_{(1)}(X_i))}{e(X_i)} - \frac{(1 - W_i) (Y_i - \mu_{(0)}(X_i))}{1 - e(X_i)} \right). \end{equation*} $$ </p> <p>Then, under flexible conditions described below, we can verify that</p> <p> $$ \begin{equation*} |\hat{\tau}_{AIPW} - \hat{\tau}^*_{AIPW}| = O_P \left( \max_w \mathbb{E} [(\hat{\mu}_{(w)}(X_i) - \mu_{(w)}(X_i))^2]^{\frac{1}{2}} \mathbb{E} [(\hat{e}(X_i) - e(X_i))^2]^{\frac{1}{2}} \right). $$ \end{equation*} </p> <p>In other words, $\hat{\tau}<em>{AIPW}$ is a good approximation for the oracle $\hat{\tau}^*</em>{AIPW}$ as long as both the outcome regressions $\hat{\mu}_{(w)}(\cdot)$ and the propensity regression $\hat{e}(\cdot)$ are reasonably accurate; and if one of them is very accurate it can tolerate the other being less so. The upshot is that, if</p> <p> $$ \begin{equation*} \max_w \mathbb{E} [(\hat{\mu}_{(w)}(X_i) - \mu_{(w)}(X_i))^2]^{\frac{1}{2}} \mathbb{E} [(\hat{e}(X_i) - e(X_i))^2]^{\frac{1}{2}} = o(n^{-1/2}), \end{equation*} $$ </p> <p>then $\hat{\tau}_{AIPW}$ is first-order equivalent to the oracle, meaning that</p> <p> $$ \begin{equation*} \sqrt{n} (\hat{\tau}_{AIPW} - \hat{\tau}^*_{AIPW}) \xrightarrow{p} 0. \end{equation*} $$ </p> <p>Now, $\hat{\tau}^*_{AIPW}$ is just an IID average, so we immediately see that</p> <p> $$ \begin{equation*} \sqrt{n} (\hat{\tau}^*_{AIPW} - \tau) \Rightarrow \mathcal{N} \left(0, V^* \right), \end{equation*} $$ </p> <p>and so whenever $\sqrt{n} (\hat{\tau}<em>{AIPW} - \hat{\tau}^*</em>{AIPW}) \xrightarrow{p} 0$ holds $\hat{\tau}<em>{AIPW}$ also satisfies a CLT as above. In interpreting the constraint, note that if $\hat{\mu}</em>{(w)}$ and $\hat{e}$ both attained the parametric “$\sqrt{n}$-consistent” rate, then the error product would be bounded as $O(1/n)$. A simple way to satisfy it is to have all regression adjustments be $o(n^{-1/4})$ consistent in root-mean squared error (RMSE), which is an order of magnitude slower than the parametric rate. Moreover, this condition doesn’t depend on the internal structure of the machine learning method used; rather, it only depends on the mean-squared error of the risk adjustments, and so justifies tuning the $\hat{\mu}_{(w)}(\cdot)$ and $\hat{e}(\cdot)$ estimates via cross-validation.</p> <h2 id="cross-fitting">Cross-fitting</h2> <p>When choosing which treatment effect estimator to use in practice, we want to attain performance as in the CLT equation and so need to make sure that first-order equivalence holds. In order to formally establish this result, it is helpful to consider the following minor modification of AIPW using cross-fitting. At a high level, cross-fitting uses cross-fold estimation to avoid bias due to overfitting; the reason why this works is exactly the same as why we want to use cross-validation when estimating the predictive accuracy of an estimator.</p> <p>Cross-fitting first splits the data (at random) into two halves $\mathcal{I}_1$ and $\mathcal{I}_2$, and then uses an estimator</p> <p> $$ \begin{equation*} \hat{\tau}_{AIPW} = \frac{|\mathcal{I}_1|}{n} \hat{\tau}_{\mathcal{I}_1} + \frac{|\mathcal{I}_2|}{n} \hat{\tau}_{\mathcal{I}_2}, \end{equation*} $$ </p> <p> $$ \begin{equation*} \hat{\tau}_{\mathcal{I}_1} = \frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1} \left( \hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \hat{\mu}_{\mathcal{I}_2}^{(0)}(X_i) + \frac{W_i (Y_i - \hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i))}{\hat{e}_{\mathcal{I}_2}(X_i)} - \frac{(1 - W_i) (Y_i - \hat{\mu}_{\mathcal{I}_2}^{(0)}(X_i))}{1 - \hat{e}_{\mathcal{I}_2}(X_i)} \right), \end{equation*} $$ </p> <p>where the $\hat{\mu}<em>{\mathcal{I}_2}^{(w)}(\cdot)$ and $\hat{e}</em>{\mathcal{I}<em>2}(\cdot)$ are estimates of $\mu</em>{(w)}(\cdot)$ and $e(\cdot)$ obtained using only the half-sample $\mathcal{I}<em>2$, and $\hat{\tau}</em>{\mathcal{I}<em>2}$ is defined analogously (with the roles of $\mathcal{I}_1$ and $\mathcal{I}_2$ swapped). In other words, $\hat{\tau}</em>{\mathcal{I}_1}$ is a treatment effect estimator on $\mathcal{I}_1$ that uses $\mathcal{I}_2$ to estimate its nuisance components, and vice-versa. This cross-estimation construction allows us to, asymptotically, ignore the idiosyncrasies of the specific machine learning adjustment we chose to use, and to simply rely on the following high-level conditions:</p> <ol> <li> <strong>Overlap</strong>: The true propensity score is bounded away from 0 and 1, such that $\eta &lt; e(x) &lt; 1 - \eta$ for all $x \in \mathcal{X}$.</li> <li> <strong>Consistency</strong>: All machine learning adjustments are sup-norm consistent, \(\sup_{x \in \mathcal{X}} |\hat{\mu}_{\mathcal{I}_2}^{(w)}(x) - \mu_{(w)}(x)|, \sup_{x \in \mathcal{X}} |\hat{e}_{\mathcal{I}_2}(x) - e(x)| \xrightarrow{p} 0.\)</li> <li> <strong>Risk decay</strong>: The product of the errors for the outcome and propensity models decays as \(\mathbb{E} \left[ (\hat{\mu}_{\mathcal{I}_2}^{(w)}(X_i) - \mu_{(w)}(X_i))^2 \right] \mathbb{E} \left[ (\hat{e}_{\mathcal{I}_2}(X_i) - e(X_i))^2 \right] = o \left( \frac{1}{n} \right),\) where the randomness above is taken over both the training of $\hat{\mu}_{(w)}$ and $\hat{e}$ and the test example $X$.</li> </ol> <p>Given these assumptions, we characterize the cross-fitting estimator by coupling it with the oracle efficient score estimator $\hat{\tau}^*$, i.e.,</p> <p> $$ \sqrt{n} (\hat{\tau}_{AIPW} - \hat{\tau}^*) \xrightarrow{p} 0. $$ </p> <p>To do so, we first note that we can write</p> <p> $$ \hat{\tau}^* = \frac{|\mathcal{I}_1|}{n} \hat{\tau}_{\mathcal{I}_1, *} + \frac{|\mathcal{I}_2|}{n} \hat{\tau}_{\mathcal{I}_2, *} $$ </p> <p>analogously. Moreover, we can decompose $\hat{\tau}_{\mathcal{I}_1}$ itself as</p> <p> $$ \begin{equation*} \hat{\tau}_{\mathcal{I}_1} = \hat{\mu}_{\mathcal{I}_1}^{(1)} - \hat{\mu}_{\mathcal{I}_1}^{(0)}, \quad \hat{\mu}_{\mathcal{I}_1}^{(1)} = \frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1} \left( \hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) + \frac{W_i (Y_i - \hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i))}{\hat{e}_{\mathcal{I}_2}(X_i)} \right), \end{equation*} $$ </p> <p>etc., and define $\hat{\mu}<em>{\mathcal{I}_1, *}^{(0)}$ and $\hat{\mu}</em>{\mathcal{I}<em>1, *}^{(1)}$ analogously. Given this buildup, in order to verify $\sqrt{n} (\hat{\tau}</em>{AIPW} - \hat{\tau}^*) \xrightarrow{p} 0$, it suffices to show that</p> <p> $$ \sqrt{n} \left( \hat{\mu}_{\mathcal{I}_1}^{(1)} - \hat{\mu}_{\mathcal{I}_1, *}^{(1)} \right) \xrightarrow{p} 0. $$ </p> <p>We now study the term by decomposing it as follows:</p> <p> $$ \begin{align*} \hat{\mu}_{\mathcal{I}_1}^{(1)} - \hat{\mu}_{\mathcal{I}_1, *}^{(1)} &amp;= \frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1} \left( \hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) + \frac{W_i(Y_i - \hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i))}{\hat{e}_{\mathcal{I}_2}(X_i)} - \mu_{(1)}(X_i) - \frac{W_i(Y_i - \mu_{(1)}(X_i))}{e(X_i)} \right) \\ &amp;= \frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1} \left( (\hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \mu_{(1)}(X_i)) \left( 1 - \frac{W_i}{e(X_i)} \right) \right) \\ &amp;+ \frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1} W_i \left( (Y_i - \mu_{(1)}(X_i)) \left( \frac{1}{\hat{e}_{\mathcal{I}_2}(X_i)} - \frac{1}{e(X_i)} \right) \right) \\ &amp;- \frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1} W_i \left( (\hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \mu_{(1)}(X_i)) \left( \frac{1}{\hat{e}_{\mathcal{I}_2}(X_i)} - \frac{1}{e(X_i)} \right) \right). \end{align*} $$ </p> <p>Now, we can verify that these are small for different reasons. For the first term, we intricately use the fact that, thanks to our double machine learning construction, $\hat{\mu}_{\mathcal{I}_2}^{(w)}$ can effectively be treated as deterministic. Thus after conditioning on $\mathcal{I}_2$, the summands used to build this term become mean-zero and independent:</p> <p> $$ \begin{align*} \mathbb{E} &amp;\left[ \left( \frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1} \left( (\hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \mu_{(1)}(X_i)) \left( 1 - \frac{W_i}{e(X_i)} \right) \right) \right)^2 \right] \\ &amp;= \mathbb{E} \left[ \mathbb{E} \left[ \left( \frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1} \left( (\hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \mu_{(1)}(X_i)) \left( 1 - \frac{W_i}{e(X_i)} \right) \right) \right)^2 \;\middle|\; \mathcal{I}_2 \right] \right] \\ &amp;= \mathbb{E} \left[ \text{Var} \left[ \frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1} \left( (\hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \mu_{(1)}(X_i)) \left( 1 - \frac{W_i}{e(X_i)} \right) \right) \;\middle|\; \mathcal{I}_2 \right] \right] \\ &amp;= \frac{1}{|\mathcal{I}_1|} \mathbb{E} \left[ \text{Var} \left[ (\hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \mu_{(1)}(X_i)) \left( 1 - \frac{W_i}{e(X_i)} \right) \;\middle|\; \mathcal{I}_2 \right] \right] \\ &amp;= \frac{1}{|\mathcal{I}_1|} \mathbb{E} \left[ \mathbb{E} \left[ (\hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \mu_{(1)}(X_i))^2 \left( \frac{1}{e(X_i)} - 1 \right) \;\middle|\; \mathcal{I}_2 \right] \right] \\ &amp;\leq \frac{1}{\eta |\mathcal{I}_1|} \mathbb{E} \left[ (\hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \mu_{(1)}(X_i))^2 \right] \\ &amp;= o_P \left( \frac{1}{n} \right) \end{align*} $$ </p> <table> <tbody> <tr> <td>by consistency (2), because $</td> <td>\mathcal{I}_1</td> <td>\approx n/2$. The key step in this argument was the 3rd equality: Because the summands become independent and mean-zero after conditioning, we “earn” a factor $1/</td> <td>\mathcal{I}_1</td> <td>$ due to concentration of iid sums.</td> </tr> </tbody> </table> <p>The second summand in our decomposition here can also be bounded similarly (thanks to overlap). Finally, for the last summand, we simply use Cauchy-Schwarz:</p> <p> $$ \begin{align*} \frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1, W_i=1} \left( (\hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \mu_{(1)}(X_i)) \left( \frac{1}{\hat{e}_{\mathcal{I}_2}(X_i)} - \frac{1}{e(X_i)} \right) \right) &amp;\leq \sqrt{\frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1, W_i=1} (\hat{\mu}_{\mathcal{I}_2}^{(1)}(X_i) - \mu_{(1)}(X_i))^2} \times \sqrt{\frac{1}{|\mathcal{I}_1|} \sum_{i \in \mathcal{I}_1, W_i=1} \left( \frac{1}{\hat{e}_{\mathcal{I}_2}(X_i)} - \frac{1}{e(X_i)} \right)^2} \\&amp;= o_P \left( \frac{1}{\sqrt{n}} \right) \end{align*} $$ </p> <p>by risk decay (3). (To establish this fact, also note that by consistency (2), the estimated propensities will all eventually also be uniformly bounded away from 0, $\eta/2 \leq \hat{e}_{\mathcal{I}_2}(X_i) \leq 1 - \eta/2$, and so the MSE for the inverse weights decays at the same rate as the MSE for the propensities themselves.)</p> <p>The upshot is that by using cross-fitting, we can transform any $o_P(n^{-1/4})$-consistent machine learning method into an efficient ATE estimator. Also, the proof was remarkably short (at least compared to a typical proof in the semiparametric efficiency literature). Stefan Wager recommends:</p> <blockquote> <p>When I talk about AIPW, I’ll implicitly assume we’re using cross-fitting unless specified otherwise. I also recommend using cross-fitting when implementing AIPW in practice.</p> </blockquote> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-10-15-augmented-ipw-and-double-robustness.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Jongmin Mun. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Best Subset Selection via a Modern Optimization Lens | Jongmin Mun</title> <meta name="author" content="Jongmin Mun"> <meta name="description" content="Lecture summary for 02-04-2026 on Best Subset Selection"> <meta name="keywords" content="Jongmin Mun, USC"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?v=1772155723"> <link rel="canonical" href="https://jong-min-moon.github.io/blog/2026/best-subset-selection-via-a-modern-optimization-lens/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Best Subset Selection via a Modern Optimization Lens",
      "description": "Lecture summary for 02-04-2026 on Best Subset Selection",
      "published": "February 4, 2026",
      "authors": [
        {
          "author": "Jongmin Mun",
          "authorURL": "https://github.com/Jong-Min-Moon",
          "affiliations": [
            {
              "name": "USC Marshall",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jongmin </span>Mun</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Study</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/coursework/">Coursework</a> <a class="dropdown-item" href="/study/papers/">Papers</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Posts<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill paper-review"> <d-title> <h1>Best Subset Selection via a Modern Optimization Lens</h1> <p>Lecture summary for 02-04-2026 on Best Subset Selection</p> </d-title> <d-article> <div class="publications" style="grid-column: text; margin-bottom: 2rem;"> <ol class="bibliography"><li> <div class="row grid-item" data-category='"optimization, statistics, machine learning"'> <div id="bertsimasBestSubsetSelection2016" class="col-sm-12"> <div class="title">Best Subset Selection via a Modern Optimization Lens</div> <div class="author"> Dimitris Bertsimas, Angela King, and Rahul Mazumder</div> <div class="periodical"> <em>The Annals of Statistics</em>, Apr 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#overview">Overview</a></div> <div><a href="#best-subset-selection-bss-problem">Best Subset Selection (BSS) Problem</a></div> <div><a href="#modern-optimization-approach">Modern Optimization Approach</a></div> <div><a href="#computational-advantages">Computational Advantages</a></div> </nav> </d-contents> <h2 id="best-subsets-current-approaches-and-limitations">Best-Subsets: Current Approaches and Limitations</h2> <p><strong>Lasso ($\ell_1$-regularization)</strong> is an extremely effective proxy for best-subset selection. Its primary strengths are:</p> <ul> <li> <strong>Computation:</strong> As a convex optimization problem, it benefits from fast, well-understood solvers.</li> <li> <strong>Theory:</strong> The statistical properties and convergence rates are well-documented.</li> </ul> <p>However, Lasso can fall short in terms of both <strong>variable selection consistency</strong> and <strong>prediction error</strong>. This is highlighted by the following risk bound</p> <p>\(\sup_{\|\beta^*\|_0 \leq k} \frac{1}{n} \mathbb{E}(\|\mathbf{X}\hat{\beta}_{L1} - \mathbf{X}\beta^*\|_2^2) \lesssim \frac{1}{\gamma^2} \frac{\sigma^2 k \log p}{n}\) This bound is tight (minimax optimal; <d-cite key="raskutti_minimax_2011"></d-cite>:)</p> <h3 id="the-impact-of-high-correlation">The Impact of High Correlation</h3> <p>A critical factor in this bound is the compatibility constant (or restricted eigenvalue) <strong>$\gamma$</strong>, which depends heavily on the design matrix $\mathbf{X}$.</p> <p>Specifically, when the correlation between columns of $\mathbf{X}$ is strong, <strong>$\gamma$ becomes very small</strong>. Because $\gamma^2$ appears in the denominator, the error bound “blows up,” leading to poor predictive performance and unstable variable selection. This sensitivity to collinearity is a well-known limitation of the Lasso framework compared to exact $L_0$ methods.</p> <h2 id="ell_0-approach">$\ell_0$ approach</h2> <p>This paper focuses on solving the <strong>$\ell_0$-constrained</strong> least squares problem to certifiable optimality: \(\min_{\beta} \frac{1}{2} \|\mathbf{y} - \mathbf{X}\beta\|_2^2 \quad \text{s.t.} \quad \|\beta\|_0 \leq k\) The cardinality constraint makes problem NP-hard. Exisiting statistical literature do not scale to problem sizes larger than p = 30.</p> <p>However, this paper integrates tools from various branches of mathematical optimization. While <strong>Mixed-Integer Programming (MIP)</strong> is a core component, the approach also leverages <strong>Discrete First-Order Methods</strong>. These methods are directly motivated by traditional first-order techniques in convex optimization and are applied to the following general problem:</p> \[\min_{\beta} g(\beta) \quad \text{s.t.} \quad \|\beta\|_0 \leq k\] <p>Here, $g(\beta)$ is a convex function that satisfies the <strong>Lipschitz gradient condition</strong>:</p> \[\|\nabla g(\beta) - \nabla g(\beta_0)\| \leq \ell \cdot \|\beta - \beta_0\|\] <p>By combining these discrete methods with modern computational techniques from MIP, the framework can handle significantly larger feature spaces than previous exact $\ell_0$ approaches while maintaining global optimality guarantees.</p> <h3 id="generalization-and-flexibility-of-the-mio-framework">Generalization and Flexibility of the MIO Framework</h3> <p>A significant advantage of the framework proposed by Bertsimas et al. (2016) is its inherent flexibility, as it can be seamlessly adapted to several variants of the best-subset regression problem. The general formulation is expressed as:</p> \[\min_{\beta} \frac{1}{2} \|\mathbf{y} - \mathbf{X}\beta\|_q^q \quad \text{s.t.} \quad \|\beta\|_0 \le k, \quad \mathbf{A}\beta \le \mathbf{b}\] <p>This adaptability allows for several powerful modeling extensions:</p> <ul> <li> <strong>Diverse Loss Functions</strong>: The parameter $q \in {1, 2}$ allows the user to switch between a <strong>least squares</strong> loss ($q=2$) and a <strong>least absolute deviation</strong> (L1) loss ($q=1$) on the residuals $\mathbf{r} := \mathbf{y} - \mathbf{X}\beta$.</li> <li> <strong>Structured Constraints</strong>: The inclusion of $\mathbf{A}\beta \le \mathbf{b}$ enables the integration of <strong>polyhedral constraints</strong>, allowing for the enforcement of specific domain knowledge or structural requirements directly within the optimization.</li> </ul> <h2 id="mixed-integer-optimization-mio">Mixed Integer Optimization (MIO)</h2> <p><strong>Mixed Integer Optimization (MIO)</strong> is a mature subfield of mathematical optimization that provides a robust framework for modeling and solving structured non-convex problems. Because the field is so well-established, we have a deep understanding of the “under-the-hood” mechanics within modern solvers.</p> <h3 id="diverse-problem-classes">Diverse Problem Classes</h3> <p>The framework is highly versatile, supporting various problem types including:</p> <ul> <li><strong>Mixed Integer Linear Optimization (MILO)</strong></li> <li><strong>Mixed Integer Second-Order Cone Programming (MISOCP)</strong></li> </ul> <h3 id="a-comprehensive-solver-ecosystem">A Comprehensive Solver Ecosystem</h3> <p>Users can choose from a wide range of powerful solvers depending on their specific needs:</p> <ul> <li> <strong>Commercial Solvers</strong>: Industry leaders include <strong>Gurobi</strong>, <strong>CPLEX</strong>, and <strong>Xpress</strong>.</li> <li> <strong>Non-commercial/Open-Source Solvers</strong>: Reliable options include <strong>SCIP</strong>, <strong>CBC</strong>, <strong>GLPK</strong>, and <strong>lpsolve</strong>.</li> </ul> <h3 id="seamless-integration">Seamless Integration</h3> <p>MIO tools are easily accessible through modern programming interfaces, allowing for flexible implementation across different environments:</p> <ul> <li> <strong>Languages</strong>: Full support for <strong>Python</strong>, <strong>R</strong>, <strong>Matlab</strong>, and <strong>Julia</strong> (specifically through <strong>JuMP</strong>). <h3 id="complexity-vs-practicality">Complexity vs. Practicality</h3> <p>While MIO is <strong>worst-case NP-hard</strong>, this theoretical classification does not mean these problems are unsolvable. The prevailing attitude in modern operations research is that NP-hardness is a “challenge accepted”. The goal is to find efficient ways to solve these problems in practice.</p> </li> </ul> <h3 id="the-mio-renaissance">The MIO Renaissance</h3> <p>We have seen a significant rise in the utility of <strong>Mixed Integer Optimization (MIO)</strong> due to dramatic improvements in both software and algorithms over the last decade. Leading solvers like <strong>Gurobi</strong> and <strong>CPLEX</strong> have demonstrated nearly a <strong>2x speedup every year</strong>, independent of hardware advancements.</p> <p>This exponential growth in computational efficiency has led major tech companies, including <strong>Google</strong> and <strong>Microsoft</strong>, to rely on Gurobi to solve their most complex large-scale optimization problems. Given this revolution in the “workhorse” tools of mathematical optimization, the paper poses a provocative and fundamental question:</p> <blockquote> <p><strong>“Why not statistics?”</strong></p> </blockquote> <p>If MIO can solve massive industrial logistics and scheduling problems, it should be leveraged to provide certifiable, global solutions to the structured non-convex problems inherent in statistical learning, such as best-subset selection.</p> <h1 id="mixed-integer-quadratic-programming-miqp">Mixed Integer Quadratic Programming (MIQP)</h1> <p>The general form of a Mixed Integer Quadratic Programming (MIQP) problem is defined as follows:</p> \[\min_{\alpha} \alpha^T \mathbf{Q} \alpha + \alpha^T \mathbf{a}\] <p><strong>Subject to:</strong></p> <ul> <li> <strong>Linear Constraints</strong>: $\mathbf{A}\alpha \le \mathbf{b}$</li> <li> <strong>Discrete Variables</strong>: $\alpha_i \in {0, 1}, \quad i \in \mathcal{I}$</li> <li> <strong>Continuous Variables</strong>: $\alpha_j \ge 0, \quad j \notin \mathcal{I}$</li> </ul> <h3 id="parameters-and-notation">Parameters and Notation</h3> <ul> <li> <strong>Input Parameters</strong>: $\mathbf{a} \in \mathbb{R}^m$, $\mathbf{A} \in \mathbb{R}^{k \times m}$, and $\mathbf{b} \in \mathbb{R}^k$.</li> <li> <strong>Quadratic Matrix</strong>: $\mathbf{Q} \in \mathbb{R}^{m \times m}$ is assumed to be <strong>positive semidefinite</strong>, ensuring the objective remains convex.</li> <li> <strong>Inequalities</strong>: The symbol “$\le$” denotes element-wise inequalities.</li> <li> <strong>Variable Set</strong>: The optimization is performed over $\alpha \in \mathbb{R}^m$, which contains both discrete indices ($\mathcal{I} \subset {1, \dots, m}$) and continuous indices. <h1 id="best-subset-selection-via--mio">Best-Subset Selection via MIO</h1> <p>To implement the best-subset selection problem within an MIO framework, the standard least squares objective is coupled with “Big-M” constraints to bridge the continuous coefficients $\beta$ and the binary indicators $z$.</p> </li> </ul> <p>The problem is formally expressed as:</p> \[\min_{\beta, z} \frac{1}{2} \|\mathbf{y} - \mathbf{X}\beta\|_2^2\] <p><strong>Subject to:</strong></p> <ul> <li> <strong>Big-M Constraints:</strong> $-M z_i \le \beta_i \le M z_i, \quad i = 1, \dots, p$</li> <li> <strong>Binary Indicators:</strong> $z_i \in {0, 1}, \quad i = 1, \dots, p$</li> <li> <strong>Sparsity Constraint:</strong> $\sum_{i=1}^{p} z_i \le k$</li> </ul> <h3 id="logical-coupling-via-big-m">Logical Coupling via Big-M</h3> <p>The parameter $M$ is a sufficiently large constant (e.g., $M = 1000$) that enforces the relationship between variable selection and estimation:</p> <ul> <li> <strong>If $z_i = 0$:</strong> The constraint forces the coefficient $\beta_i$ to be exactly zero.</li> <li> <strong>If $z_i = 1$:</strong> The coefficient $\beta_i$ is free to take any value within the practical range $[-M, M]$.</li> </ul> <h3 id="certifiable-optimality">Certifiable Optimality</h3> <p>A primary advantage of this approach is the <strong>Certificate of Optimality</strong> provided by MIP technology. Unlike heuristic methods, MIO solvers systematically tighten the gap between the <strong>upper bound</strong> (the best feasible solution found) and the <strong>lower bound</strong> (the theoretical best possible value).</p> <p>This allows for controlled convergence; the solver can be stopped once a pre-defined <strong>optimality gap</strong> (e.g., 1% or 5%) is achieved, providing a rigorous guarantee on the solution’s proximity to the global optimum.</p> <p><img src="assets/img/MIP_trajectory_typical.png" alt="MIP trajectory"></p> <p>The paper claims that for $n \leq 1000, p \leq 1000$, the MIO approach can solve the best-subset selection problem to good quality within a few minutes, but certificates of optimality can take longer.</p> <h3 id="improvement-1-implied-inequalities">Improvement 1. Implied Inequalities</h3> <p>A standard strategy in integer programming is to introduce <strong>implied inequalities</strong> that, while mathematically redundant, significantly tighten the feasible region for the solver’s relaxation. This leads to much tighter lower bounds and faster pruning of the branch-and-bound tree.</p> <h4 id="sos-1">SOS-1</h4> <p>Any feasible solution to formulation above will have $(1 − z_i)\beta_i = 0$ for every $i \in {1, \dots, p}$. This constraint can be modeled via integer optimization using Specially Ordered Sets of Type (SOS-1), as follows: $(1 − z_i)\beta_i = 0 \Leftrightarrow (\beta_i, 1 − z_i) : \text{SOS-1}$, for every $i = 1, \dots, p$. This removed the specification of big-M. However, in practice, it is better to keep it. Even more, it’s better to add additional $\ell_1$ constraint. As a result, we have:</p> \[\begin{aligned} &amp; \min_{\boldsymbol{\beta}, \mathbf{z}} &amp; &amp; \frac{1}{2} \boldsymbol{\beta}^T (\mathbf{X}^T \mathbf{X}) \boldsymbol{\beta} - \langle \mathbf{X}' \mathbf{y}, \boldsymbol{\beta} \rangle + \frac{1}{2} |\mathbf{y}|2^2 \ \\ &amp; \text{s.t.} &amp; &amp; (\beta_i, 1 - z_i) : \text{SOS-1}, \quad i = 1, \dots, p \ \\&amp; &amp; &amp; z_i \in {0, 1}, \quad i = 1, \dots, p \ \\&amp; &amp; &amp; \sum_{i=1}^{p} z_i \leq k \ \\&amp; &amp; &amp; -M_U \leq \beta_i \leq M_U, \quad i = 1, \dots, p \ \\&amp; &amp; &amp; |\boldsymbol{\beta}|_1 \leq \mathcal{M}_\ell \end{aligned} \quad (2.5)\] \[\quad \sum_{i=1}^{p} z_i \leq k\] <p>The original $\ell_0$ problem is strengthened by incorporating these additional norm constraints:</p> \[\min_{\beta} \|\mathbf{y} - \mathbf{X}\beta\|_2^2\] <p>\(\text{s.t.} \quad \|\beta\|_0 \leq k\) \(\|\beta\|_{\infty} \leq \delta_{11}, \quad \|\beta\|_1 \leq \delta_{21}\) \(\|\mathbf{X}\beta\|_{\infty} \leq \delta_{12}, \quad \|\mathbf{X}\beta\|_1 \leq \delta_{22}\)</p> <p>By bounding the coefficients and their projections ($\mathbf{X}\beta$) in both $L_1$ and $L_\infty$ spaces, the solver can discard suboptimal regions of the search space much more aggressively.</p> <hr> <h3 id="2-polyhedral-outer-approximation-simplifying-curves-into-straight-lines">2. Polyhedral Outer Approximation: Simplifying Curves into Straight Lines</h3> <p>Nonlinear optimization is difficult and slow to solve directly. To overcome this, we approximate the “curvy” problem using a sequence of <strong>Mixed-Integer Linear Programs (MILP)</strong>, which modern solvers can handle with extreme efficiency. Think of it like approximating a circle by drawing a many-sided polygon around it; as you add more sides (straight lines), your approximation gets closer and closer to the actual curve.</p> <h4 id="the-mathematical-framework">The Mathematical Framework</h4> <p>Modern solvers perform best when working with linear or piecewise-linear structures. To handle a quadratic (squared) objective like least-squares within this linear framework, we use a <strong>polyhedral outer approximation</strong> $\mathcal{P}$:</p> \[\mathcal{P} \subset \left\{ \beta : \frac{1}{2} \|\mathbf{y} - \mathbf{X}\beta\|_2^2 \leq t \right\}\] <p>This approach breaks down the global “curvy” loss into individual, manageable pieces using auxiliary variables $t_i$ and residuals $r_i$:</p> <ul> <li> <strong>Aggregate Loss</strong>: We ensure the sum of individual errors stays below a total threshold $t$: $\sum_{i \in [n]} t_i \leq t$.</li> <li> <strong>Individual Residual Bounds</strong>: Each specific error point $r_i^2$ is bounded by its own $t_i$: $r_i^2 \leq t_i, \quad i \in [n]$.</li> <li> <strong>Residual Definition</strong>: The residual is the difference between our prediction and the actual data: $\mathbf{r} = \mathbf{y} - \mathbf{X}\beta$.</li> </ul> <h4 id="why-this-works">Why This Works</h4> <p>Instead of trying to solve the complex squared term all at once, the algorithm surrounds the quadratic curve with a series of <strong>linear tangential cuts</strong>—flat planes that touch the curve. This allows the solver to leverage high-speed linear programming engines to iteratively refine the solution. Each time the solver finds a point outside the true curve, it adds a new “flat” constraint to slice away that suboptimal region, eventually “wrapping” the solution tightly around the global optimum.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2026-02-04-best-subset-selection.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Jongmin Mun. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
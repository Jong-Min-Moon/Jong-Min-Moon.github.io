<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Lecture 16: Canonical Gradient and Efficient Influence Curve | Jongmin Mun</title> <meta name="author" content="Jongmin Mun"> <meta name="description" content="Notes by Rachael Phillips for PB HLTH 290, Spring 2019"> <meta name="keywords" content="Jongmin Mun, USC"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?v=1772159654"> <link rel="canonical" href="https://jong-min-moon.github.io/blog/2026/canonical-gradient-efficient-influence-curve/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Lecture 16: Canonical Gradient and Efficient Influence Curve",
      "description": "Notes by Rachael Phillips for PB HLTH 290, Spring 2019",
      "published": "February 20, 2026",
      "authors": [
        {
          "author": "Rachael Phillips",
          "authorURL": "",
          "affiliations": [
            {
              "name": "UC Berkeley",
              "url": ""
            }
          ]
        },
        {
          "author": "Jong Min Moon",
          "authorURL": "https://github.com/Jong-Min-Moon",
          "affiliations": [
            {
              "name": "USC Marshall",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jongmin </span>Mun</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Study</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/coursework/">Coursework</a> <a class="dropdown-item" href="/study/papers/">Papers</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Posts<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill "> <d-title> <h1>Lecture 16: Canonical Gradient and Efficient Influence Curve</h1> <p>Notes by Rachael Phillips for PB HLTH 290, Spring 2019</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#setting">Setting</a></div> <div><a href="#insight-the-geometry-of-pathwise-derivatives">Insight: The Geometry of Pathwise Derivatives</a></div> <div><a href="#goal">Goal</a></div> <div><a href="#parametric-submodels-and-scores">Parametric submodels and scores</a></div> <div><a href="#tangent-space-t-p">Tangent Space, T(P)</a></div> <div><a href="#problem-with-standard-directional-derivative-of-target-parameter">Problem with standard directional derivative of target parameter</a></div> <div><a href="#pathwise-derivative">Pathwise derivative</a></div> <div><a href="#class-of-gradients">Class of gradients</a></div> <div><a href="#canonical-gradient-is-projection-of-gradient-on-tangent-space">Canonical gradient is projection of gradient on tangent space</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>An asymptotically linear estimator with influence curve equal to the efficient influence curve is optimal in the sense that there is no other asymptotically linear estimator with influence curve with a smaller variance. We call this estimator asymptotically efficient.</p> <h2 id="setting">Setting</h2> <ol> <li> <p><strong>Data and Model:</strong> \(O_1, \dots, O_n \overset{iid}{\sim} P_0 \in \mathcal{M}.\) Here, \(\mathcal{M}\) denotes the statistical model, which is the collection of all possible probability distributions \(P\) that could generate the data.</p> </li> <li> <p><strong>Target Parameter:</strong> The target parameter is defined as a functional (or operator) \(\Psi: \mathcal{M} \to \mathbb{R}\). This mapping takes a probability distribution \(P\) as input and returns a scalar value representing a specific feature of that distribution (e.g., the mean, the risk difference).</p> </li> <li> <p><strong>Estimand:</strong> The true value of the parameter, often denoted as \(\psi_0 = \Psi(P_0)\), is the <strong>estimand</strong>. This is an unknown quantity because the true data-generating distribution \(P_0\) is unknown.</p> </li> </ol> <h2 id="insight-the-geometry-of-pathwise-derivatives">Insight: The Geometry of Pathwise Derivatives</h2> <p><strong>The Problem: The “Straight Line” Fallacy.</strong> We want to measure the sensitivity (“steepness”) of the functional \(\Psi\) at the distribution \(P\). In standard calculus, we would simply take a derivative along a straight line (\(P + \epsilon Q\)). However, the space of probability distributions is curved, not flat (it is not a vector space). If we try to walk in a straight line off of \(P\), we immediately land in “invalid territory” (e.g., generating negative probabilities or measures that do not sum to one).</p> <p><strong>The Solution: The Curve-Drawing Machine.</strong> To stay within the valid distribution space, we approach \(P\) along smooth curves. We utilize a <strong>parametric submodel</strong>, constructed by a <strong>curve-drawing machine</strong> \(P_\epsilon^h\).</p> <ul> <li> <strong>Input:</strong> We feed it a “drawing parameter” \(h\) (a function), which determines the style or direction of the curve.</li> <li> <strong>Action:</strong> As we vary \(\epsilon\), the machine draws a series of dots (probability distributions) inside the model space.</li> <li> <strong>Output:</strong> The collection of these dots forms the curve \(\mathcal{M}_h(P)\). By construction, every dot on this curve is a valid probability distribution passing through \(P\) at \(\epsilon=0\).</li> </ul> <p><strong>The Mechanism: The Chain Rule Analogy.</strong> We want to calculate how the parameter \(\Psi\) changes as we move along this curve. We can understand this via a <strong>Chain Rule Analogy</strong>. While the formal calculus of functionals is more complex, the intuition parallels standard calculus (\(\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}\)):</p> \[\underbrace{\frac{d}{d\epsilon} \Psi(P^h_\epsilon)}_{\text{Total Change}}\bigg|_{\epsilon=0} \approx \underbrace{\text{"Operator Change"}}_{\frac{d\Psi}{dP}} \cdot \underbrace{\text{"Curve Change"}}_{\frac{dP}{d\epsilon}}\] <p>However, mathematically, the components are defined more precisely in the Hilbert space \(L_2(P)\).</p> <p><strong>Conclusion.</strong> The beauty of this approach is that we can separate the geometry of the model from the target parameter. We can pre-compute the “curve part” (the Score \(S_h\)) purely based on the submodel. When we combine it with the Gradient via the inner product, we recover the pathwise derivative we need to study efficiency.</p> <h2 id="goal">Goal</h2> <p>Our primary objective is to estimate the unknown quantity \(\psi_0 = \Psi(P_0)\) and to understand the fundamental limits of estimation accuracy. The properties of the functional \(\Psi\) itself dictate the difficulty of the estimation problem.</p> <ul> <li> <strong>Local Perturbations:</strong> To quantify this difficulty, we analyze the behavior of \(\Psi\) under local perturbations. We ask: “If the true distribution \(P_0\) changes slightly, how much does the parameter \(\Psi(P_0)\) change?”</li> <li> <strong>Derivatives and Variance:</strong> This concept is analogous to a derivative in calculus. The “steepness” of this functional derivative (formally captured by the <em>Efficient Influence Function</em>) determines the <em>Information Bound</em>. A “steeper” functional implies that the parameter is more sensitive to fluctuations in the data, resulting in a harder estimation problem (higher minimum variance).</li> </ul> <h2 id="parametric-submodels-and-scores">Parametric submodels and scores</h2> <p><strong>Motivation: Valid Directions.</strong> When defining the derivative of a target parameter, we cannot simply look at arbitrary perturbations \(P + \epsilon h\) (as in standard calculus). The resulting object \(P + \epsilon h\) might not be a valid probability distribution (e.g., it might not integrate to 1 or could be negative). Therefore, we must restrict our attention to perturbations within the space of valid probabilities. We achieve this by defining <em>parametric submodels</em>.</p> <h3 id="parametric-submodel-given-h">Parametric submodel, given h</h3> <p>For a specific path \(h\), we define a one-dimensional parametric submodel passing through the true distribution \(P\):</p> \[\mathcal{M}_h(P) = \{ P^h_{\epsilon} : \epsilon \in (-\delta, \delta) \} \subset \mathcal{M}\] <p>This submodel (collection of distributions) is a curve within the large model \(\mathcal{M}\) such that:</p> <ul> <li>At \(\epsilon = 0\), the distribution is the true data-generating distribution: \(P^h_{\epsilon=0} = P\).</li> <li>For \(\epsilon&gt;0\), we move away from \(P\) by \(\epsilon\) in the direction \(h\), while remaining inside the model \(\mathcal{M}\).</li> <li>The specific value of \(\delta\) is not critical. We are only interested in the behavior of the submodel in the immediate neighborhood of \(\epsilon=0\).</li> <li>Form of \(P^h_{\epsilon}\) is quite flexible. Not necessarily \(P+ \epsilon h\). We are allowed to invent any path \(P_\epsilon\) we want, as long as it passes through the true model \(P\) at \(\epsilon=0\). There is no single “correct” way to draw a line through a probability distribution.</li> </ul> <h3 id="score-given-h">Score, given h</h3> <p>The only direction we care about each submodel, \(\mathcal{M}_h(P)\), is its score. For a path \(h\), its score \(S_h\) is defined as a transformation of an observation:</p> \[S_h(O)=\left . \frac{d}{d\epsilon}\log dP_{\epsilon}^h/dP(O)\right |_{\epsilon=0}\] <table> <tbody> <tr> <td>Notice that the score is defined as usual. We take the log of the density that is defined with respect to \(P\) itself. In other words, you choose the path where all the probability distributions are of the same nature as \(P\) itself so that you can define \(\frac{dP_{\epsilon}}{dP}\). Then we have a collection of densities because \(\frac{dP_{\epsilon}}{dP} = p_{\epsilon}^h\) so we have that $$S_h(O)=\frac{d}{d\epsilon}\log p_{\epsilon}^h</td> <td>_{\epsilon=0}$$.</td> </tr> </tbody> </table> <h3 id="tangent-space-and-hilbert-space">Tangent space and Hilbert space</h3> <h4 id="the-tangent-set-class-of-scores-class-of-h">The Tangent Set (Class of Scores): class of h</h4> <p>We consider the class of all parametric submodels \(\{\mathcal{M}_h(P) : h \in \mathcal{H}\}\), indexed by a set of paths \(\mathcal{H}\). Let \(\mathcal{S} = \{ S_h : h \in \mathcal{H} \}\) be the collection of all score functions generated by these paths. <strong>We should be careful about \(\mathcal{H}\)</strong>.</p> <ul> <li> <strong>Richness of \(\mathcal{H}\):</strong> We choose the index set \(\mathcal{H}\) to be sufficiently “rich” to ensure coverage. This ensures that the set of scores \(\mathcal{S}\) captures all possible local directions in which we can perturb \(P\) while remaining within the constraints of the model \(\mathcal{M}\).</li> <li> <strong>Tangent Space:</strong> The set \(\mathcal{S}\) (specifically, the closure of its linear span) is formally called the <strong>Tangent Space</strong> of the model at \(P\). includes any function that can be approximated by limit of elements of the linear span</li> </ul> <h4 id="the-hilbert-space-l_02p">The Hilbert Space \(L_0^2(P)\)</h4> <p><strong>Scores as Random Variables.</strong> Scores are measurable functions of the data \(O \sim P\). Therefore, they are random variables with specific properties:</p> <ul> <li> <strong>Mean Zero:</strong> \(\mathbb{E}_P[S(O)] = 0\).</li> <li> <strong>Finite Variance:</strong> \(\mathrm{Var}_P[S(O)] &lt; \infty\).</li> </ul> <p><strong>Hilbert Space.</strong> We define \(L^2_0(P)\) as the Hilbert space containing all such mean-zero, square-integrable functions of \(O\) (thus they are mostly correlated)</p> \[L^2_0(P) = \{ f(O) : \mathbb{E}_P[f(O)]=0, \, \mathbb{E}_P[f(O)^2] &lt; \infty \}\] <p>with inner product defined as the covariance (since they are centered):</p> \[\langle f, g \rangle_P = \mathbb{E}_P[ f(O)g(O) ] = \mathrm{Cov}_P(f,g)\] <p>Scores belong to \(L_0^2(P)\).</p> <p><strong>Orthogonality.</strong> In this space, two functions are <strong>orthogonal</strong> (\(f \perp g\)) when their corresponding random variables are <strong>uncorrelated</strong>. Since our limiting distribution is Gaussian, later this will also mean independence.</p> <p><strong>Projection.</strong> Projection is the bread and butter for the Hilbert space.</p> <ul> <li>Let \(S\) be an element of \(L^2_0(P)\),</li> <li>Let \(H\) be a sub-Hilbert space of \(L^2_0(P)\). For example, tangent space at \(P\).</li> <li>Then the projection \(\Pi(S\mid H)\) of \(S\) onto \(H\) is a unique element defined by <ol> <li>\(\Pi(S\mid H)\in H\): being an element in \(H\),</li> <li>\(S-\Pi(S\mid H) \perp H\): \(S\) minus projection is uncorrelated with any element in \(H\).</li> </ol> </li> </ul> <h3 id="example">Example</h3> <p><strong>Model.</strong> \(\mathcal{M}\) is nonparametric. Here we define it as a collection of all probability distributions which have densities.</p> <p><strong>Direction \(h(o)\).</strong></p> <ul> <li>\(h\) is also a function of \(o\)</li> <li>\(h(o)\) represents the “shape” of the perturbation</li> <li>If \(h(o)\) is positive, we increase the probability of observing \(o\)</li> <li>If \(h(o)\) is negative, we decrease it.</li> <li>We pick \(h(o)\) such that \(h\) uniformly bounded and \(\mathbb{E}_Ph(O)=0\). This broad definition is equivalent to defining \(\mathcal{H}\)</li> </ul> <p><strong>Submodel.</strong> We <strong>define</strong> \(P_\epsilon^h\) so that \(dP_{\epsilon}(o)=(1+\epsilon h(o)) dP(o)\). Defined via densities.</p> <ul> <li>In semi-parametric theory, we are allowed to invent any path \(P_\epsilon\) we want, as long as it passes through the true model \(P\) at \(\epsilon=0\). There is no single “correct” way to draw a line through a probability distribution.</li> <li>Submodel check 1: density integration to 1</li> </ul> \[\int dP_\epsilon(o) = \int (1 + \epsilon h(o)) dP(o) = \underbrace{\int 1 \, dP(o)}_{=1} + \epsilon \underbrace{\int h(o) \, dP(o)}_{= E_P[h(O)] \text{ should be 0}}\] <p>Intuition: To add probability mass to one area (where \(h &gt; 0\)), we must steal it from another area (where \(h &lt; 0\)) to keep the total mass constant.</p> <ul> <li>Submodel check 2: nonnegativity. Let’s think of a worst case scenario: at some observation \(o\), \(h(o)\) takes its most negative possible value; \(h(o) = -\|h\|_\infty\). Then the scaling factor becomes \(1 + \epsilon (-\|h\|_\infty)\). We need this factor to stay non-negative:</li> </ul> \[1 - \epsilon \|h\|_\infty \ge 0 \iff 1 \ge \epsilon \|h\|_\infty \iff \epsilon \le \frac{1}{\|h\|_\infty}\] <p>Therefore, if we restrict \(\epsilon\) to be smaller than \(\delta = 1/\|h\|_\infty\), i.e. \(\epsilon\in (-\delta,\delta)\) with \(\delta=1/\|h\|_{\infty}\), this is a submodel \(\mathcal{M}_h(P)\).</p> <p><strong>Score.</strong> This construction perfectly yields the score \(h\). By the construction \(dP_{\epsilon} = (1+\epsilon h) dP\):</p> \[S(O) = \frac{d}{d\epsilon} \log \big( \frac{(1+\epsilon h(O)) dP(O)}{dP(O)} \big) \bigg|_{\epsilon=0}\] <p>The derivative of \(\log(u)\) is \(u'/u\):</p> \[S(O) = \frac{h(O)}{1+\epsilon h(O)} \bigg|_{\epsilon=0}\] <p>Set \(\epsilon=0\):</p> \[S(O) = \frac{h(O)}{1} = h(O)\] <p><strong>Scores.</strong> \(\mathcal{S}\) is all \(h\in L^2_0(P)\) with \(\|h\|_{\infty}&lt;\infty\).</p> <h2 id="tangent-space-tp">Tangent Space, T(P)</h2> <ul> <li>Let \(T(P)\subset L^2_0(P)\) be the closure of the linear span of the set of scores \(\mathcal{S}\) of our class of paths.</li> <li>This is a sub-Hilbert space of \(L^2_0(P)\).</li> <li>It is called the tangent space at \(P\).</li> <li>The tangent space for a <em>nonparametric</em> model is the whole \(L^2_0(P)\). We say that the model is locally saturated at \(P\).</li> </ul> <p>We have that the score is an element of the Hilbert space and we have a collection of scores that correspond with this class of paths, generating a sub-Hilbert space of \(L_0^2(P)\). We might take any linear combination of all the scores and the closure (any function you can approximate as an a limit of such linear combinations of all these scores is also additive) and that creates a sub-Hilbert space, \(H\) of \(L_0^2(P)\). \(H\) is the tangent space corresponding with this class of paths.</p> <h2 id="problem-with-standard-directional-derivative-of-target-parameter">Problem with standard directional derivative of target parameter</h2> <ul> <li>We want to define a type of differentiability of \(\Psi:\mathcal{M}\rightarrow\mathbb{R}^d\).</li> <li>We could use the definition of a directional derivative in direction \(h\):</li> </ul> \[d\Psi(P)(h)=\left . \frac{d}{d\epsilon}\Psi(P+\epsilon h)\right |_{\epsilon =0}\] <ul> <li>However, \(P+\epsilon h\) is not a path within \(\mathcal{M}\), so this could be ill defined.</li> <li>Therefore, we define a derivative along paths that are submodels of \(\mathcal{M}\).</li> </ul> <h2 id="pathwise-derivative">Pathwise derivative</h2> <ul> <li>The pathwise derivative is defined as:</li> </ul> \[d\Psi(P)(S_h)=\left . \frac{d}{d\epsilon}\Psi(P_{\epsilon}^h)\right |_{\epsilon =0}\] <ul> <li>This is linear operator in its score \(S_h\).</li> <li>Thus, \(d\Psi(P):L^2_0(P)\rightarrow\mathbb{R}^d\) is a real valued linear operator on a Hilbert space \(L^2_0(P)\).</li> </ul> <h3 id="pathwise-differentiability-and-gradient">Pathwise differentiability and gradient</h3> <ul> <li>\(\Psi:\mathcal{M}\rightarrow\mathbb{R}^d\) is pathwise differentiable at \(P\) if its pathwise derivative is a <strong>bounded</strong> linear operator.</li> <li>By the Riesz-representation theorem, then \(d\Psi(P):L^2_0(P)\rightarrow\mathbb{R}^d\) can be represented as an inner product of gradient with score:</li> </ul> \[d\Psi(P)(S_h)=E_P D(P)(O)S_h(O)= \langle D(P),S_h\rangle_P\] <ul> <li>\(D(P)\) is called a gradient of the pathwise derivative.</li> </ul> <h2 id="class-of-gradients">Class of gradients</h2> <ul> <li>A gradient is not necessarily unique.</li> <li>Let \(T(P)^{\perp}=\{S\in L^2_0(P):P\perp T(P)\}\) be orthogonal complement of \(T(P)\).</li> <li>If \(D(P)\) is a gradient, then \(D(P)+S\) with \(S\in T(P)^{\perp}\) is also a gradient.</li> </ul> <h2 id="canonical-gradient-is-projection-of-gradient-on-tangent-space">Canonical gradient is projection of gradient on tangent space</h2> <ul> <li>There is one unique gradient \(D^*(P)\in T(P)\) in the tangent space.</li> <li>This is called the canonical gradient.</li> <li>The set of all gradients is \(D^*(P)+S\) with \(S\in T(P)^{\perp}\).</li> <li>If \(D\) is gradient, then canonical gradient \(D^*(P)\) is the projection of \(D(P)\) onto tangent space.</li> </ul> <h3 id="example-1">Example</h3> <ul> <li>\(O=T\), \(\mathcal{M}\) nonparametric model, \(\Psi(P)=P(T&gt;5)\).</li> <li>\(dP_{\epsilon}(T) =(1+\epsilon S(T))dP(T)\), \(S(T)\) is score.</li> <li> \[\left . \frac{d}{d\epsilon}\Psi(P_{\epsilon}^h)\right |_{\epsilon =0} =E_P D(P)(T)S_h(T)\] </li> </ul> <p>where gradient</p> \[D(P)(T)=I(T&gt;5)-\Psi(P)\] <h3 id="nonparametric-model-has-only-one-gradient">Nonparametric model has only one gradient</h3> <ul> <li>This gradient \(D(P)\) is also the canonical gradient.</li> </ul> <p>\(T(P)=L_0^2(P)\) so the orthogonal complement of the tangent space is empty meaning you cannot add to the canonical gradient anything to create more gradients.</p> <h3 id="finding-canonical-gradient-in-non-saturated-models">Finding canonical gradient in non-saturated models</h3> <ul> <li>First find a gradient \(D(P)\) by computing the pathwise derivative for each path \(=E_P[D(P)(O)S(O)]\).</li> <li>The canonical gradient equals the projection of \(D(P)\) onto the tangent space \(T(P)\):</li> </ul> \[D^*(P)=\Pi(D(P)\mid T(P))\] </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Jongmin Mun. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>